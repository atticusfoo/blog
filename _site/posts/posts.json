[
  {
    "path": "posts/2021-04-11-visual-analytics-assignment/",
    "title": "Visual Analytics Assignment",
    "description": "Exploring and Protoyping visualizations methods for Understanding News Corpus.",
    "author": [
      {
        "name": "Atticus Foo",
        "url": "https://public.tableau.com/profile/atticusfoo#!/"
      }
    ],
    "date": "2021-04-11",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview of Problem Statement: Difficult to sense-make news headlines in digital landscape\r\nProject Objective: Provide readers with useful snapshots of news headlines\r\nOur Data Source: Local News Reports in 2020\r\nLiterature review:\r\nImportance of analyzing unstructured news data\r\nImpact of News Media on our lives\r\nImpact of Social Media and Echo Chambers\r\nLimited resources for visualising unstructured data\r\n\r\nApproach\r\n1. Data extraction and Wrangling\r\nData Extraction\r\nData Exploration Before Cleaning\r\nScoping the dataset\r\nDataText Pre-Processing and cleaning\r\n\r\n2.Topic Modeling Methods\r\nWhy Latent Dirichlet Allocation (LDA)\r\nCriticisms of LDA\r\nDifferent methods and packages for LDA\r\nIssues faced\r\nPackage Selected: text2Vec\r\nVisualizing Results\r\n\r\n3.Methods for Presenting Snapshot\r\n3.1.Objective 1: Explore - Sorting through and identifying key events in large corpus of news\r\n3.2.Objective 2: Discover - Providing greater detail and context about a topic / news source\r\n3.2.1 USing Sentiment\r\n3.2.2 USing WordNetworks\r\n3.2.2 USing Word Co-occurences\r\n\r\n3.4.Objective 3: Detect - Providing alerts to unusual patterns in news analysis\r\n\r\n\r\nSummary of observations from exploration of R Packages\r\nProposed Sketch of Project Module 2: Discovering Topics and it’s context\r\nClosing Reflections\r\n\r\n\r\n\r\n\r\nFigure 1: Data Re-presented.\r\n\r\n\r\n\r\nOverview of Problem Statement: Difficult to sense-make news headlines in digital landscape\r\nMaking sense of the news in today’s landscape is extremely difficult because of:\r\nThe volume of information available to you via digital channels;\r\nSocial media bubbles may create echo chambers that only serve you only one aspect of news making it difficult to understand the full context;\r\nNews can be manipulated based on phishing (creating duplicates) and inflated engagement metrics campaigns.\r\nThis can be useful as news headlines can have very real impact on not just personal sense-making but also cause tangible effects. This can be seen from how media reporting can contribute to the public’s understanding of safe distancing measures during COVID-19 to how it can influence share prices on the stock market. The following blogpost is to establish the project’s objectives, explore current practices, and prototype methods in order to help readers navigate the digital news landscape.\r\nProject Objective: Provide readers with useful snapshots of news headlines\r\nAs such, the team’s goal is to build an accessible dashboard that can help readers of local news sites (both mainstream and non-mainstream) quickly navigate and understand a large corpus of news. This will be done by applying statistical methods to sort large volumes of unstructured data. The intended result is different snapshots that would allow readers to:\r\n[Explore] Sort through and identify key events in the large corpus of news events by:\r\nTime period;\r\nTopic and;\r\nKeyword\r\n\r\n[Discover] Understand in greater detail the context around each theme and news source by\r\nSentiment and;\r\nWord structure\r\n\r\n[Detect] Be alerted to unusual patterns\r\nIdentify anomalous events and articles based on engagement.\r\n\r\nAs such, while the presentation of the ‘snapshot’ may appear simple, the methods that would derive these insights are not. The approach taken for this project is based on a mixed of deep research of current statistical methods, domain knowledge of the local news reporting landscape and rigorous data wrangling.\r\nOur Data Source: Local News Reports in 2020\r\nThe data selected for analysis is from 1 January 2020 to 31 December 2020. These local news sites are a mix of mainstream (Straits Times, Channel News Asia, AsiaOne) and non-mainstream (Mothership, MustShareNews, The Independent) sites. A few others such as (TODAY Online, The Online Citizen) were initially selected but removed from our observations. This will be explained at the data wrangling phase.\r\nPrrtData extracted from Prrt is extremely useful for the project as it provides an easy tool for the extraction of the article headline, corresponding URL and Facebook Engagement metrics (Likes, Shares and Comments). In the absence of actual webviewership data, Facebook engagement data is often used as a proxy as the dataset is more easily accessible. Facebook is also arguably the most widely used social media platform in Singapore> it is also still used by major news sites for information dissemination as well as resource for analysis and understanding of the virality of a news article and topic.\r\nLiterature review:\r\nImportance of analyzing unstructured news data\r\nWhile preparing for the project, we researched four different areas to understand the key areas that needed to be addressed. The main challenge of the project is that of dealing with large volumes of unstructured data (224,351 news headlines and 2,692,224 observations). There are also limited libraries we can call upon in R as text analytics packages for R on CRAN are limited in number. Visualization packages that avoid simple word cloud comparisons are far fewer in number. This is compounded by the fact that the project is very niche as it is set in the local context, and as such, would have little case study options readily available.\r\nDespite these constraints, the team is of the opinion that the project is a worthy one as there is an increasing need to help navigate the digital news landscape. As mentioned in the earlier section, news can have a very large and tangible impact on our lives and the increasing availability of data visualization tools within the realms of journalism can help readers make sense of the stories that are out there. However, as identified by Data Journalism, there is not enough being done to take advantage of this.\r\nFortunately, as news media tended to follow standardized schools of reporting (e.g. AP style Book), there was research available (predominantly) on different methods used to analyse western media. These methods could then be adopted for analysing Singaporean media. For instance, text cleaning and sentiment analysis could easily adopt the English packages as they were unlikely to contain colloquial lingua franca that was most used on local social networking sites. Furthermore, the use of standardised stylebooks for news reporting would help the Latent Dirichlet Model which the team intends to use, as the model relies on the probabilities of similar words across similar topic distributions.\r\nImpact of News Media on our lives\r\nAs identified by Bhargava, Bishop and Zuckerman, there is clear evidence of the influence of the news media and its news reporting. They posit that building tools can help readers quickly analyse content, influence and the spread of news. In turn, they will be better equipped to respond to the growing impact of visual (and viral) news. This group’s earlier study also demonstrated that mobile app usage frequency can be correlated to news events and reporting (Chee, Nam, Foo 2020).\r\nKouivunen Niemi and Masoodian echo this in their recent article, visualising narrative patterns in online news media. They argue that the new media and its reports can have “widespread repercussions in the public perception of past and present phenomena” and proposed “temporal visualizations for examining differences in media narrative patterns over time and across publications”. This is provide a tool or method to simply to report findings in an “easy to understand but effective way”. They also go further to lament how these type of (text) studies are “almost invariably presented in tables, or using simple graphs such as bar charts or line charts”.\r\nCo-occurrences across different textsImpact of Social Media and Echo Chambers\r\nIn recent years, increasing amounts of research (the echo chamber effect on social media have also point to the dangers of social media companies inadvertently creating echo-chambersin order to maximise engagement metrics. They do this by presenting readers content that they are likely already biased towards, thus increasing the propensity (Platforms like Facebook are designed to profit for human’s confirmation bias that they continue engagement with the social media platform.\r\nTo combat this, researchers propose that readers pay attention the news source to ensure authenticity and the corresponding engagement metrics in order to determine if an article has been manipulated to reach a wider audience (e.g. when the engagement ratios for a content does not appear to be natural. This happens typically when ‘likes’ or ‘shares’ are mobilised. As such, we intend to provide a range of 6 sources that readers can easily access when they are looking for information related to a topic. They are then able to explore the context surrounding a topic (i.e. sentiment and co-occurring words) and use engagement metrics to identify if there are anomalous articles.\r\nLimited resources for visualising unstructured data\r\nA commonly used method for visualising text is the use of word clouds . They often aim to represent the most commonly used words in a corpus by size but have limited utility at best (crf. when word clouds are not enough, and mis-representative of the data at [worst]https://towardsdatascience.com/word-clouds-are-lame-263d9cbc49b7). Often, they include common verbs that do very little to explain word in the context of the corpus of data (crf. Why word clouds harm insights.\r\nPhoto Credit: Shelby TempleOther interesting methods explored for text visualization build on Venn and Euler diagrams.\r\nEuler DiagramThey include work by Rodgers, Stapleton & Chapman (Visualizing sets with linear diagrams which demonstrate the intersection where words are shared by different sources or topics.\r\nVisualizing sets with linear diagramsA similar approach was adopted by Luz and Masood (a comparison of linear and mosaic diagrams for set visualization.\r\nComparison of different scalings of mosaic and lienar diagramsHowever, these methods were not entirely applicable for our project as these visualizations benefitted from a small number of documents that could be extensive in length. Our dataset however, consists of over 200,000 documents that were all very short in length (a news headline typically has up to 30 words).\r\nOther than visualising topics and their word occurrences in other texts, we also intended to learn methods to visualise the temporal evolution of news topics as we intended to allow readers to explore how news evolved across a full year in 2020. This, we felt, would be interesting as the COVID-19 pandemic allowed for us to track the evolution of a news topic across nearly 12 months. This was somewhat of an irregularity in news media as the typical news cycle operates on a much shorter cycle of 48 hours (or less). While researching on this, we came across interesting methods used by Sheidin & Lanir (Visualising Spatial-Temporal Evaluation of News Stories.\r\nNews of Iran firing ballistic missles around the worldThe visualisation presents an idea for a system that is able to analysis the spread and volume of news episodes across space and time. However, this is not entirely applicable to the dataset used in our project because the number of news sources in the study are low (6) and all based in Singapore. Another mitigating, but more minor factor, is the lack of time data in the dataset.\r\nApproach\r\nBased on the research, our proposed approach for the project would be to create three separate modules that would allow users to 1) explore the corpus to identify key events 2) understand the key events / topics in greater detail and 3) to be alerted to unusual patterns. While the eventual presentation will appear to be a simple (and ideally effective) report card of the media situation, it will be grounded at the backend by statistical methods and rigorous data wrangling.\r\n1. Data extraction and Wrangling\r\nData Extraction\r\nAs mentioned above, the dataset that we have extracted is taken from Prrt. This is because Prrt is unique to local news media publishers in Singapore (and Malaysia) and allows us to extract news headlines together with its corresponding social media engagement. For other types of news extraction, there are R packages that offer news headline extraction. These include Guardian API Media News and Newsmap.\r\nAn interesting package for news extraction and analysis is NewsFlow as it allows for tracking of the flows between offline and online news. It is an interesting case study of how quickly news spreads from the offline sphere to the online one, or vice versa.\r\nNewsFlowHowever, as the scope of the project is primarily concerned with online news, we will not be able to utilize the NewsFlow package for our analysis.\r\nData Exploration Before Cleaning\r\nBefore we started on data cleaning, we want to explore the dataset to identify if there are any potential issues (such as low or missing values) in the dataset. R provides many packages that can help do this (e.g. Dplyr, GGbetweenstats, Data Explorer and Janitor.\r\nWe will use Data Explorer to perform preliminary data exploration, to identify which areas to go into further depth. Data explorer allows for very quick EDA and feature reporting. For this step, we will need to install Tidyverse and DataExplorer.\r\ninstall.packages(“DataExplorer”, “tidyverse”, dependencies = TRUE)\r\nlibrary(DataExplorer, tidyverse)\r\n#read csv file using tidyverse from your local source folder\r\nmedia_explore <- read_csv(“Data/all_media.csv”) \r\n#check data for missing values\r\ndata(media_explore) \r\nplot_missing(media_explore) \r\n#create a pdf report \r\ncreate_report(media_explore) \r\nDataexplorerResults from the Data Explorer also show little issues with missing values as the dataset is relatively complete and that the data fields are in the correct data types (e.g. chr and num)\r\nDataexplorerDataexplorerScoping the dataset\r\nWhile we wanted a good mix of news sites in our dataset for analysis, we had to be judicious in pruning four sites from our dataset. These were Rice Media, The Online Citizen, TODAY Online and Yahoo SG. These sites were removed for the following reasons: - Rice Media did not publish every day and contributed to a very small amount of articles published in 2020; - The Online Citizen had long a periods of missing data from the months of April 2020 to August 2020; - TODAY Online tended to be similar in style and scope as Channel News Asia (being that they both operated as Mediacorp’s English news websites); - Yahoo SG was removed as they performed primarily as a content aggregator with the bulk of their content republished from news wires such as Reuters.\r\nTo further refine the dataset that we wanted to analyze, we decided to set a threshold on engagement data, and include in the dataset, articles with at least 10 interactions. Doing this reduces the number of articles in the dataset by 131,235 (or close to 58%) and removes articles that were: - Removed quickly after A/B testing; - Removed quickly after editorial corrections; - Republished from international news wire agencies and therefore had low to no engagement.\r\nAs we are focused with news articles that have some reach, we removed articles with lower than 10 Facebook engagement from our analysis. A remaining 93,116 articles was available for analysis.\r\ntest <-Cleaned %>%\r\n  filter(Total>10)%>%\r\n  select(c(1,10))\r\ntest\r\nDataText Pre-Processing and cleaning\r\nBefore we can begin text processing, we will need to employ text cleaning to reduce noise to optimize results. Examples of ‘noise’ include special characters, punctuation, common words and typographical errors. During this process, words will also be converted to lower case characters so that same words can all be identified by the computer as the same entity. Useful packages for doing this is the StringR function within the TidyVerse package, Spacyr which utilizes Python function using Reticulate, Quanteda, Qdap or TM. The method we are using below uses the Stringr package. Stringr is a useful package for working with strings and regular expressions using its pattern matching function and can be used for character manipulation to do text pre-processing.\r\n\r\nlibrary(stringr)\r\n#using stringr and textclean for cleaning\r\nmedia_processed$Cleaned<-tolower(media_explore$Text)%>%#convert to lowercase\r\n  replace_contraction() %>% #lengthening words (eg,isn't -> is not)\r\n  replace_word_elongation() %>% #reducing informal writing (eg,heyyyyyyy -> hey)\r\n  str_replace_all(\"[0-9]\", \" \") %>% #removing numbers\r\n  str_replace_all(\"[[:punct:]]\",\"\")%>%#remove punctuation\r\n  str_replace_all(\"covid|wuhan virus\",\"coronavirus\")%>%#word substitution\r\n  str_squish()%>% #reduce repeated whitespace \r\nstr_trim#removes whitespace from start and end of string\r\n#check data \r\nview(media_processed)\r\n#we will first need to create a Dataframe source from a dataframe\r\nmedia <- DataframeSource(media_explore) \r\n###################### not in use ############################\r\n#we can now create the corpus using tm \r\ncorpus <- Corpus(media)\r\n#remove words that at not English \r\nskipWords <- function (x) removeWords (x, stopwords(“english”)) \r\n#changing all words to lower case\r\ncorpus <- tm_map(corpus, FUN = tm_reduce, tmFuns = list (tolower))\r\n#remove \r\ncorpus < - corpus, FUN = tm_reduce, tmFuns = list(skipWords, removePunctuation, stripWhitespace, removeNumbers, stemdocument))\r\n\r\nA popular method in data cleaning for text is the use of Term Frequency – Inverse Document Frequency or TD-IDF which is a pre-processing method that helps surface ‘interesting’ terms in a corpus and reduces importance of commonly used words that appear frequently in the corpus. However, as identified earlier, the nature of standardized style of news reports around common events will inevitably engender the occurrence of common terms. These ‘common terms are therefore important and should not be reduced in ‘weightage’ when analyzing a corpus of text. Because of this, we will not be adopting the TF-IDF method for text cleaning for the purposes of this project and will proceed on to the next step, Topic Modeling.\r\n2.Topic Modeling Methods\r\nWhy Latent Dirichlet Allocation (LDA)\r\nTo help readers make sense of key events or topics in the large corpus of data, the project will take advantage of the Latent Dirichlet Allocation (LDA) model. This is because LDA is a generative statistical model “that allows for sets of observations to be explained by unobserved groups that explain why some part of the data are similar”. The LDA method has also been successfully adopted for other fields such as banking.\r\nCriticisms of LDA\r\nAlthough LDA has been criticized for issues with overfitting and lacks an intrinsic method for choosing a number of topics it is a method that is extremely advantageous for the purposes of our study. This is because the method itself is unsupervised and allows for the dashboard to assist readers in identifying topics that are ‘unknown-unknowns’ even though some of the topic models derived may not be immediately clear. As we are unable to ‘predict’ news events, we are unable to pre-define topics for analysis and presentation in the snapshot. As the LDA method is unsupervised and can handle ‘unknown-unknowns’, the proposed dashboard is thus scalable and reproducible as it will be able to handle new datasets containing new or future news articles.\r\nDifferent methods and packages for LDA\r\nAfter selecting the LDA method, we identified different methods and packages that can be used. These include the traditional LDA, word2vec and the newer text2vec. Both word2vec and text2vec methods build on top of the LDA model and are less resource demanding and has a smaller memory footprint because it does not need to perform a lookup over an associative array. It does this with feature hashing that maps features into a more compact space (crf. original research paper by Yahoo).\r\nIssues faced\r\nCreating topic models using TidyText and its associated packages (Quanteda and TM) was not ideal for 220k documents as the document term matrix would be prohibitively large. Each iteration of LDA modeling using this method would take longer (e.g. 5,000 iterations took about 5hrs on our laptops).\r\nWe were also not able to properly harness memory allocation packages such as (Big Memory and Parallel. The processes were still lengthy even after we removed infrequent words from the Document Term Matrix. More importantly, the results were not easily reproducible as the LDA model is based on random seeds, it would return a different result (if we did not set a seed) after every 5 hours of running it.\r\nTidy Text Topic ModelingPackage Selected: text2Vec\r\nDue to the reasons outlined above, we proceeded with the Text2Vec package for Topic Modelling. The steps on topic modelling using text2vec can be found below:\r\n#install package \r\ndevtools::install_github('dselivanov/text2vec\r\nlibrary(text2vec) \r\n#tokenize words\r\ntokens = media_processed$cleaned) \r\ntokens = word_tokenizer(tokens)\r\nit = itoken(tokens, ids = media_processed$docid, progressbar = FALSE)\r\nv = create_vocabulary(it)\r\n#remove very common and uncommon words \r\nv = prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)\r\nvectorizer = vocab_vectorizer(v)\r\n#constructing a Document Term Matrix \r\ndtm2 = create_dtm(it, vectorizer, type = \"dgTMatrix\")\r\n# Creating LDA for 20 topics\r\nlda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)\r\ndoc_topic_distr = \r\n  lda_model$fit_transform(x = dtm2, n_iter = 1000, \r\n                          convergence_tol = 0.001, n_check_convergence = 25, \r\n                          progressbar = FALSE)\r\n# identifying the proportion of word distribution\r\nbarplot(doc_topic_distr[1, ], xlab = \"topic\", \r\n        ylab = \"proportion\", ylim = c(0, 1), \r\n        names.arg = 1:ncol(doc_topic_distr))\r\nDoing this will give the distribution for the topic distribution for the first document#getting top 10 words for each topic based on lambda 0.4\r\nlda_model$get_top_words(n = 10, topic_number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 19L, 20L), lambda = 0.4)\r\nWe will now get the LDA topic model results, based on 10 keywords for 20 topics using the lambda 0.4.\r\n20 Topics#apply learned model to new data \r\nit = itoken(test$Cleaned[4001:5000], tolower, word_tokenizer, ids = test$docid[4001:5000])\r\nnew_dtm =  create_dtm(it, vectorizer, type = \"dgTMatrix\")\r\nnew_doc_topic_distr = lda_model$transform(new_dtm)\r\n#Check for perplexity. The lower the score, the better\r\nperplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)\r\nVisualizing Results\r\nAfter retrieving the topics from text2vec, we would now need to visualize the results. One interesting visualization for topic modeling was the termite model created by a team at Stanford University. As seen in the screenshot below, the visualization is clean yet simple, and clearly communicates the size and overlap of keywords between topics.\r\nTermiteThis visualization below is the alternative using LDAviz.\r\n Visualization below may not be visible in Chrome as chrome does not allow running of local files.\r\nWe want to save the topics derived for further analysis using the code below. This will allow engagement data derived from the articles to be visualized in an aggregated fashion as topics, instead of per article (which is not ideal as there are 96k articles).\r\ngroup_by(day)%>%\r\nadd_column(topic = topic ,.before = 'Date’))\r\n3.Methods for Presenting Snapshot\r\nTo achieve the objective of presenting a useful snapshot or report card of the media situation, we explored different packages such as [SunburstR(https://cran.r-project.org/web/packages/sunburstR/index.html), GGVIS , Esquisse and D3PartitionR which can read large volumes of data quickly (using FREAD).\r\nD3PartitionR ExampleHowever, after running some tests, we found the visualizations for SunrburstR and D3Partition to be unsuitable. This is because of the size of the documents found in the corpus being too large to visualize using these methods. The speed of interactions was slow, and the visualization outcomes poor due to the variety of colors used for the range of variables found in the dataset.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAs such, despite the packages ability to quickly read data (eg. using FREAD) and their ability to present topline data in an interactive and easy to understand manner, it was not suitable for the purposes of our project.\r\n3.1.Objective 1: Explore - Sorting through and identifying key events in large corpus of news\r\nThe alternative found was using a new package called CorporaExplorer. Using the package creates a Shiny App that allows us to quickly input a corpus of documents that can be easily searched by date or keyword. The output is an interactive heatmap “where each tile represents one document” as well as each document’s corresponding metadata. Using the data we have loaded, we can easily prepare the corpus for visualization using this package.\r\ninstall.packages(“corporaexplorer”)\r\nlibrary(corporaexplorer)\r\n#preparing the data \r\ncorpus_explorer <- \r\n  media_explore,\r\n  date_based_corpus = TRUE,\r\n  grouping_variable = \"Topic\", #this is skipped if date is TRUE\r\n  within_group_identifier = \"Source\", #this is also skipped if date is TRUE \r\n  columns_doc_info = c(\"Topic\", \"Headline\", \"URL\", \"Source\", \"Total\", \"Likes\", \"Shares\", \"Comments\"), #appending the relevant metadata from the dataset\r\n  corpus_name = \"Local Media Coverage in 2020\", #naming the dashboard\r\n  use_matrix = TRUE, \r\n  matrix_without_punctuation = TRUE,\r\n  tile_length_range = c(1, 25),\r\n  columns_for_ui_checkboxes = TRUE,\r\nWith the data prepared, we can now run the Shiny App to explore the corpus\r\nexplore( \r\n  corpus_explorer,\r\n  search_options = list(optional_info = TRUE), #option for keywords in search\r\n  ui_options = list(font_size = \"10px\"), #size of font in app\r\n  plot_options = list(max_docs_in_wall_view = 100000) #maximum number of documents in the list \r\n)\r\nThe screenshot above demonstrates the corporaexplorer using the dataset of local media articles published in 2020The Coporaexplorer package is able to handle the 91k articles quickly and allows for readers to search the entire corpus by (up to 5) keywords. Selecting a specific day in the Coporaexplorer plot allows readers to then discover more about the selected article / document. This includes the corresponding engagement data regarding the article’s reach and URL and allows readers to meet the goal of quickly sorting through a large corpus of news events by time, topic and keyword.\r\nMore detail regarding the module for Objective 1 can be found at the project blog here.\r\n3.2.Objective 2: Discover - Providing greater detail and context about a topic / news source\r\n3.2.1 USing Sentiment\r\nThe second objective to use statistical methods to provide greater detail and context around each topic and news source can be done through sentiment analysis. This will allow readers to understand the value of the sentiment attached to the topic or media source and observe its changes across time.\r\nThere are several sentiment analysis packages available including syuzhet and sentimentr. Lexicons that are available for sentiment analysis include “Bing”, “AFINN” and [“NRC”])(https://hoyeolkim.wordpress.com/2018/02/25/the-limits-of-the-bing-afinn-and-nrc-lexicons-with-the-tidytext-package-in-r/).\r\ninstall.packages(“sentimentr”, “GGally”, “glue”, “reshape2”)\r\nlibrary(“sentimentr”, “GGally”, “glue”, “reshape2”)\r\n#unnest tokens for analysis \r\nmedia_words <- unnest_tokens(media_explorer, word, text) \r\nview(media_words) \r\n#use bing lexicon to get media sentiment \r\nmedia_sentiment_new <- media_words %>%\r\n  inner_join(get_sentiments(\"bing\")) %>%\r\n  count(source, date, sentiment) %>%\r\n  spread(sentiment, n, fill = 0) %>%\r\n  mutate(sentiment = positive - negative)\r\n#use ggplot to plot sentiment over time \r\nggplot(media_sentiment_new, aes(date, sentiment, fill = source)) +\r\n  geom_col(show.legend = FALSE)\r\nOverall sentiment analysisArticle sentiment was most in late January, presumably about the same time the first cases of COVID-19 were found in Singapore – but later trended upwards towards the end of the year. From this analysis, readers can infer that the overall sentiment and pandemic situation improved over the span of the year.\r\nYou can change the sentiment plot to show specific media by using ggplot’s facet wrap feature using the code below:\r\n#use ggplot to plot sentiment over time \r\nggplot(media_sentiment_new, aes(date, sentiment, fill = source)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~source, ncol = 2, scales = \"free_x\") #use facet to show media sentiment across different sources \r\nOverall sentiment analysis by mediaBased on the chart, we can see that the Straits Times and Channel News Asia publishes more articles with negative keywords from the ‘Bing’ lexicon. This could also give insight to the type of news in their respective news coverage (e.g. news about accidents, crime, death tend to feature more negative keywords) as full fledged news sites could cover a wider range of news stories compared to lifestyle news focused new sites such as Must Share News. To better understand this hypothesis, we can use the word cloud functions and word structure (network and n-gram) analysis found in the later section of this blog.\r\nWe are also able to use word cloud to visualize positive and negative words used most frequently in media coverage.\r\nSentiment Word CloudAs mentioned earlier, the word cloud visualization here is not particularly useful as it provides very little value as to what the words’ context are. In the visualization above, it does a bit more to separate negative and positive words but this can be a bit arbitrary. For instance, “Trump” here is seen as positive when it refers to the verb “trump” rather than the former President Donald Trump.\r\nFinally, we are able to also chart news headline sentiment to the SGX to identify if there are any correlations between news sentiment and SGX stock prices. To do this, we will first need SGX data which can be downloaded from Yahoo Finance.\r\ninstall.packages(\"gridExtra\", \"plotly\", \"dygraphs\", \"xts\", \"zoo\")\r\nlibrary (\"gridExtra\", \"plotly\", \"dygraphs\", \"xts\", \"zoo\")\r\n\r\n#read downloaded SGX data\r\nTeststock3 <- read_csv(\"Data/sgx.csv\")\r\n#format date\r\nDate=teststock3$Date <- as.Date(as.character(teststock3$Date,\"%Y-%m-%d\"))\r\n#check date format \r\nclass(Date)\r\n#join sentiment and SGX datasets by date \r\nteststock3 <- teststock3 %>%\r\n  left_join(media_sentiment_new, by = c(\"Date\" = \"date\"))     \r\nview(teststock3)\r\n#convert data to xts format for dygraph\r\nDate=teststock3$Date <- as.xts(ts(start = c(2020-1-1), endc(2020teststock3$Date)\r\nOpen=teststock3$Open <- as.numeric(na.locf(teststock3$Open))\r\nClose=teststock3$Close <- as.numeric(na.locf(teststock3$Close))\r\nHigh=teststock3$High <- as.numeric(na.locf(teststock3$High))\r\nLow=teststock3$Low <- as.numeric(na.locf(teststock3$Low))\r\nSentiment=teststock3$sentiment <- as.numeric(na.locf(teststock3$sentiment))\r\n#bind coloums and convert to xts format required by Dygraph\r\nz=cbind(Open,Close,High, Low, Sentiment)\r\nnewdata=xts(z,Date)\r\n#plot graph using dygraph\r\ndygraph(newdata, main = \"SGX VS Sentiment of Local News Headlines\") %>%\r\n  dyEvent(\"2020-01-23\", \"First COVID-19 Case in SG\", labelLoc=\"bottom\")%>% #plotting events\r\n  dyEvent(\"2020-01-29\", \"First Travel Restrictions to SG\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-03-27\", \"Circuit Breaker Phase 1 starts\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-05-19\", \"Circuit Breaker Phase 1 Ends\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-07-10\", \"Polling Day\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-08-09\", \"National Day\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-11-09\", \"Biden confirmed as Winner\", labelLoc=\"bottom\")%>%\r\n  dyShading(from = \"2020-03-27\", to = \"2020-5-19\", color = \"#FFE6E6\") %>%\r\n  dyHighlight(highlightCircleSize = 5, \r\n              highlightSeriesBackgroundAlpha = 0.2,\r\n              hideOnMouseOut = FALSE) %>%\r\ndyRangeSelector()\r\n\r\n\r\n\r\nThe resulting plot from Dygraph provides useful interactive features that allows readers to pan and select the time period for analysis. They are also able to mouseover key data points to derive the corresponding SGX prices. To understand the tangible impact news events and sentiment has on the stock market, we conducted a multiple linear regression to observe the results. The codechunk for this can be found below:\r\n#create columns for testing MLR\r\nallCols <- colnames(teststock3)\r\nregCols <- allCols[!allCols %in% c(\"Open\",\"High\",\"Low\", \"Adj Close\")]\r\nregCols <- allCols[!allCols %in% c(\"Open\", \"High\", \"Low\")]\r\n#create regression formula \r\nregCols <- paste(regCols, collapse = \"+\")\r\nregCols <- paste(\"Close~\",regCols, collapse = \"+\")\r\nregCols <- as.formula(regCols)\r\n#call MLR \r\nTeststock3.lm <- lm(regCols, data = teststock3) \r\nSummary(teststock3.lm)\r\nMLR ResultsThe results for the multiple regression model demonstrate that there is significant collinearity between negative media sentiment and the closing stock prices of the SGX daily. As seen in the results, news events and headlines should be analyzed in greater detail because they can have very real and tangible impact.\r\n3.2.2 USing WordNetworks\r\nAnother method explored for allowing users to read in greater detail, the different topics and news sources, is the analysis and visualization of word structure. This can be done by analyzing the word graph or word networks relating to the topic or media source. Word analysis using statistical methods can be very use in helping to track and visualize the evolution of news, and demonstrate how differently each media has covered a topic.\r\nUsing R packages such as tidytext, dplyr, igraph and ggraph, we can plot the word networks related to different topics in the corpus.\r\ninstall.packages(“tidytext”, “dplyr”, “igraph”, “ggraph”)\r\nlibrary(“tidytext”, “dplyr”, “igraph”, “ggraph”)\r\n#creating bigrams\r\nmedia_bigrams <- media_explore %>%\r\n  unnest_tokens(bigram, text, token = \"ngrams\", n=2)\r\nmedia_bigrams\r\n#sorting bigrams \r\nmedia_bigrams %>%\r\ncount(bigram, sort = TRUE)\r\n#splitting bigrams and removing stop words \r\nbigrams_separated <- media_bigrams %>%\r\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\r\nbigrams_filtered <- bigrams_separated %>%\r\n  filter(!word1 %in% stop_words$word) %>%\r\nfilter(!word2 %in% stop_words$word)\r\n\r\n#new bigram counts \r\n# new bigram counts:\r\nbigram_counts <- bigrams_filtered %>% \r\n  count(word1, word2, sort = TRUE)\r\nbigram_counts\r\n#graphing word network  \r\nbigram_counts\r\nbigram_graph <- bigram_counts %>%\r\n  filter(n > 5) %>%\r\n  graph_from_data_frame()\r\nbigram_graph\r\nset.seed(2017)\r\nggraph(bigram_graph, layout = \"fr\") +\r\n  geom_edge_link() +\r\n  geom_node_point() +\r\ngeom_node_text(aes(label = name), vjust = 1, hjust = 1)\r\nWord NetworkUsing the word network plotted above allows readers to identify the words that were frequently used together and gives readers more context to the topic (ie. GE2020). However, this method is not the most intuitive and its presentation left a lot to be desired.\r\nReferenced earlier in the blog, is Masood’s work that we felt was very useful in visualizing co-occurring words across different media sources. Visualizing this could be very useful as readers can quickly infer if certain keywords or entities have been left out when analyzing topics.\r\nCo-occurrences across different textsUnfortunately, this visualization of presenting sets is more useful when the number of documents in the text is small. Visualizing this over 96k articles will be less helpful, even after aggregation into different topics. An alternative approach of visualizing co-occurrence is explore in the next section.\r\n3.2.2 USing Word Co-occurences\r\nFinally, using the tri-grams identified in the text, we are able to visualize how any two words in a selected corpus is used, where they are similar and where they diverge. Research into empirical linguistics have identified how the frequency of words occurring together can give insight and meaning to the treatment of a topic and the ‘closeness’ of association between entities. This can help readers understand the context to how words in media coverage was used when compared to each other.\r\nFor this segment, we were inspired by the work of Chris Harissson and Emil Hvitfeldt. We will need to call on the packages of purrrlyr for this next visualization. The code below is attributed to Emil Hvitfeldt.\r\ninstall.pacakges(“purrrlyr”)\r\nlibrary(purrrlyr)\r\nn_word <- 20\r\nn_top <- 150\r\nn_gramming <- 3\r\n#creating trigrams\r\ntrigrams <- tibble(text = media_explore$Text) %>%\r\n  unnest_tokens(trigram, text, token = \"ngrams\", n = n_gramming)\r\n#select the start words you want to compare\r\nstart_words <- c(\"covid\", \"coronavirus\")\r\nAs seen in the visualization above, the word network can be elegantly displayed and compared with each other to see how they unfold more often in media coverage. From the visualization, we can infer that the word PAP is more often related to a wider range of words in media coverage compared to WP. We hope to integrate this function as a module in the Shiny Dashboard for the project.\r\nThe proposed sketch and detail regarding the module for Objective 2 can be found in the later section of this blog here.\r\n3.4.Objective 3: Detect - Providing alerts to unusual patterns in news analysis\r\nTo meet objective 3 for the project, we researched how technology companies such CrowdTangle and Newswhip used engagement figures in order to predict the likelihood articles go viral. Newswhip utilizes a method of measuring velocity by tracking the speed of engagements and measures if it is overperforming when compared to the average interaction speed of other articles from the same publisher.\r\nThis can be useful for us to visualize anomalies in time series using packages in R. Sorting this by media type or by topic, can help to identify if there are key articles or events that need to be explored first, amongst the influx of articles that being disseminated at any point of time.\r\nMore detail regarding the module for Objective 3 can be found at the project blog here (url)\r\nSummary of observations from exploration of R Packages\r\nTo summarize, we will be adopting packages for three different modules to meet our objectives of helping readers to better understand news events in a large corpus of headlines, create snapshots that are based on rigorous statistical methods to understand quickly the context surrounding a topic and finally to see red-flags / or anomalous instances in the news corpus.\r\nProposed Sketch of Project Module 2: Discovering Topics and it’s context\r\nBased on the research work, methods and proto-typing found above the proposed dashboard for the Project’s Module that will help readers contextualize news topics and sources can be found in the sketch below.\r\nSketch for proposed moduleThe intended module seeks to provide media sentiment using the ‘bing’ lexicon and give insight into topics by allowing them to observe the tri-grams most frequently used with the topic keyword. Users will also be able to select the topic or media source in order to create a ‘snapshot’ that would present them with the relevant information.\r\nClick on the respective links to find out more about modules 1 and 3. The integrated Shiny dashboard for the modules will be presented at a later date.\r\nClosing Reflections\r\nWhile there are many aspects to explore to help make sense of the news corpus, we have selected packages and methods that would best suit a large volume of unstructured data and contain interactive user functions to allow them to quickly navigate the corpus and create snapshots around topics they are interested in.\r\nFuture work could include advance methods for dynamic topic modeling, which would be helpful in identifying how a topic evolve across time (e.g. from Wuhan Virus, to Coronavirus to Covid-19) as well as building on word-graphing and word network techniques for the identification of duplicate articles or ‘fake news’. Improvements can also be made to the user experience by customizing CRAN packages that are used for the purposes of this project.\r\nThank you for reading up to this point.\r\nThis blog is a data visualization assignment for the MITB programme at the Singapore Management University.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-visual-analytics-assignment/images/newsheader.jpg",
    "last_modified": "2021-04-11T18:49:06+08:00",
    "input_file": "visual-analytics-assignment.utf8.md"
  },
  {
    "path": "posts/2021-03-21-my-dataviz-makeover-3/",
    "title": "Dataviz Makeover 3",
    "description": "Visualisation Makeover on spatio-temporal analysis of conflict in South-East Asia using Tableau",
    "author": [
      {
        "name": "Chee Kah Wen Gerald",
        "url": {}
      }
    ],
    "date": "2021-03-21",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Introduction\r\n2. Critque on Clarity and Aesthetics2.1 Proposed Design\r\n\r\n3. Preparation 3.1 World Map \r\n3.2 Analysis of Events \r\n3.3 Treemap \r\n3.4 Tooltip creation \r\n3.5 Dashboard \r\n\r\n4. Observations from Makeover Visualisation \r\n\r\n1. Introduction\r\nThe Armed Conflict Location & Event Data Project (ACLED) is a non-governmental organization specializing in disaggregated conflict data collection, analysis, and crisis mapping. As of 2021, ACLED has recorded more than a million individual events across the word. The ACLED team conducts analysis to describe, explore, and test conflict scenarios, making both data and analysis open for use by the public.\r\nFIGURE 1: VISUALISATION DONE BY RESEARCH SCIENTISTBased on the above visualisations, the task is to make a critque on the Clarity and Aesthetics of the charts, providing an alternative visualisation and listing down the steps to ensure repoducibility.\r\n2. Critque on Clarity and Aesthetics\r\nClarity\r\nAesthetics\r\nEach data point represents the count of event that occurred, however the title of the visualisation makes no indication of this.\r\nEach event is too clustered, the reader is unable to distinguish how many events happened at that location due to overlapping of data points.\r\nVisualisation on the left does not have a strong message. i.e A high number of battles occured, but theres no indication of severity or extent of damaged caused by event.\r\nAccording to the legend,orange denotes ‘Explosions/Remote Violence’ events and red is used to denote ‘Protest’ events. The tones are too similar to each other thus the reader is unable to easily distinguish the events.\r\nIt its evident that certain locations within Myanmar are of interest due to the number of attacks, However there is no naming of the province or area, we are unable to further utilise the current visualisation to derive further insights or cross reference with external sources. This can be resolved by using the ‘ADMIN’ columns from the data set to allow for interactivity when a particular area is moused-over.\r\nSince the locations are not clearly defined and resolution issues, the data points might cross the boundaries of where the event actually occurred. A chloropleth map with density of of events would be a better visualisation in this case.\r\nTitle indicates years of attack from 2015-2020, there is no indication of when each event took place. Reader is left to assume that the visualisation includes all events from that time period.\r\nSince it is assumed that the left visualisation includes all events from 2015-2020, a trend line does not support the visualisation well because spikes and troughs represents the change in frequency of attacks. A better visualisation would be a running total of the number of events occurring through the years.\r\nTitle of the right visualisation has no indication that the line chart represent count of attacks by type and year.\r\nColor legend to the immediate right of visualisation might confuse the reader as we would expect each line to be color coded according to the legend. However, this legend is used in reference to left most visualisation.\r\n‘Count of sheet 1’ as y-axis title is confusing as we are left to assume through the line chart that there were many changes to ‘sheet 1’ through the years.\r\nThe y-axis scale is not standardised, thus,it might be misleading the reader to think that one event type occured more frequently than the other.\r\nInteractivity\r\nWith respect to Interactivity, we are unable tell much about the functionality of the dashboard.\r\nVisualisation has been filtered to display events occurring in Myanmar, uncertain if this allows for single values or multi-values. This might also cause difficulty in using the dashboard especially if choosing to display multiple countries for comparison, or if the reader would want to compare countries by attack type.\r\nA better solution to the dashboard would be to apply interactivity to toggle between multiple parameters. Interactivity would allow you to filter the different event types,by year or by the related parties.This is because the scale and intensity of these conflicts are different by geograpical region and you may not want to see them all at once.\r\nWe can include tooltips to reference other sheets or data values as well, for instance, adding a tooltip, we can see the main actors of conflict by each event,casualty inccured or a brief summary of the event that occured, allowing for a more conclusive story telling.\r\n2.1 Proposed Design\r\nPROPOSED VISUALISATION IN TABLEAUTaking reference from fellow peers, specifically Mike Cisneros, the aesthetics of his visualisation are quite eye-catching to the reader. I have opted to adopt a similar approach to showcase the visualisation\r\nThe colour selection: red tones to signify the types of events occuring With a proper colour legend.Since Red is used, a dark background will compliment the reds to have a more significant impact in when conveying the repercussions of the conflict.\r\nProperly spelled titles to give readers a sense of confidence in the researcher who prepared the presentation.\r\nFilters added, giving the reader a choice on what data he/she would like to exclude for analysis,particularly important when dealing with daily time series data\r\n3. Preparation \r\nThe original source of the data can be found at The Armed Conflict Location & Event Data Project\r\nSteps\r\nData Preparation\r\nAfter downloading the xlsx file, the file is imported into Tableau where we will verify the contents of the file together with the ACLED Codebook which contains identifiers for the columns.Dataset has been renamed to ‘ACLED S.E.A 2010-2020’\r\n\r\nSince not all columns are useful for analysis, click on manage metadata button to display column headers in a list view. Selected columns unnecessary for analysis will be hidden.(‘ISO, EVENT_ID_CNTY, EVENT_ID_NO_CNTY, YEAR, TIME_PRECISION, ASSOC_ACTOR_1, INTER1, ASSOC_ACTOR_2, INTER2, REGION, ADMIN1,ADMIN2, ADMIN3,SOURCE, SOURCE SCALE, GEO_PRECISION, TIMESTAMP’ columns have been hidden from analysis)\r\n\r\nAfter the above steps, we will convert the data types to appropriate ones (eg. Location as Geographic role(County)).\r\n\r\nFor the ‘Interaction’ Column, as the values are numerical, they have been converted to string data types for alias creation. Their numeric values are cross-referenced with the ACLED codebook.\r\n\r\nOf note, the codebook does not have a corresponding value for interaction id ‘70’.As such it has been coded to follow the the ACLED naming convention of ‘SOLE CIVILIAN ACTION’.To note, all the string values are in uppercase, this will be changed through tableau coding in later steps.\r\n\r\nBased on the information in the codebook column ’Location’has their geographic types changed toto city geographic type\r\n\r\nNext we extract the cleaned data for easier reference and rename it ‘Cleaned_data.csv’\r\n\r\n3.1 World Map \r\nSteps\r\nVisualisation for World Map\r\nIn a new sheet,I did a basic EDA to see which event type had highest number of fatalities, this would be used as a data point thus affecting visibility on the map visualisation.The data has been sorted in descending order.\r\n\r\nFor the world map, we first create a hierarchy by dragging the ‘Location’ Measure name into ‘Country’\r\n\r\nLongitude and Latitude measure values are dragged into the columns and row inputs.Since tableau detects them as values and are aggregated, we will turn off the ‘Aggregate Measures’ setting in the ‘Analysis’ Ribbon\r\n\r\nFrom the ‘Map Layers’ function, I adjusted the style from ‘light’ to ‘Dark’ and ticked the ‘Country Names’ and ’Country borders setting\r\n\r\nTo populate the map with color and labels, the ‘Fatalities’ were added to the size and The ‘event type’ field is added into the color button.\r\n\r\nBased on the EDA done, I sorted the the event types based on descending order of fatalities.This is needed as there maybe data points that would overlap and thus not be visible\r\n\r\nNext, I adjusted the size of the circles to the 2nd interval as shown.This reflects the number of fatalities by size.\r\n\r\nColor of events have been adjusted according to this color scheme.\r\n\r\nNext, I added a date filter for a range of ‘event dates’ and the ‘event types’.\r\n\r\nNext, I added a Country filter.Filter is displayed and changed to a drop down list of multiple values.\r\n\r\nNext, I edited the title of the fatalities legend.\r\n\r\nNext, I edited the title of the Visualisation as shown.\r\n\r\nWe finally link the sheets that are using the data values shown.\r\n\r\nThe World Map is finally visualised as seen here.\r\n3.2 Analysis of Events \r\nSteps\r\nVisualisation for Analysis\r\nFrom the sheet used to create the EDA as mentioned earlier. I will use that sheet to visualise the number of events.\r\n\r\nDrag the ‘Event date’ into column input,‘Country’ and ‘count’ into row input.’Event date’ is changed to display by ‘day’ format\r\n\r\nHold ‘Ctrl’ button on keyboard and drag the ‘DAY(Event date)’ from column values into filter.Show filter\r\n\r\nNext, Drag the ‘count’ of records into size button.\r\n\r\nNext we edit the title of the visualisation to better represent the data displayed\r\n\r\nFrom above, the title will be used as the indicator in the dashboard.Thus, we will remove the axis labels\r\n\r\nTo match the dark layout of the map for the dashboard,I will format this sheet color to black.\r\n\r\nFor the fonts, colour has been changed to Grey to allow it to be displayed clearly\r\n\r\nThe Time Series analysis is finally visualised as seen here.\r\n3.3 Treemap \r\n\r\nThe next step is to include a treemap for visualising discrete variables vs discrete variables.In this case, this would be the ‘Actor1’ against ‘Actor2’. This will help to visualise the main actors of conflicts through the countries. First, we drag the count of records into color and size buttons in the marks. Next we drag ‘Actor1’ and ‘Actor2’ into the text buttons. Grey has been chosen as the color of choice for this visualisation to reflect the sombre tone of conflict and death. The we include ‘interaction’,‘count’ and ‘sum of fatalities’ to be represented in text.\r\n3.4 Tooltip creation \r\nI will now go through the steps of adding the tooltips to each sheet for better presentation of information\r\nSteps\r\nTooltip creation\r\nIn the sheet ‘Fatalities by Countries and Event type’, I placed ‘notes’,‘country’ and ‘location’ into the tooltip button.\r\n\r\nIn the sheet ‘No of Events in Country by Event Type’ , i added a reference to sheet ‘Treemap of Actor1 vs Actor2’ into the tooltip\r\n\r\n3.5 Dashboard \r\nWe are now ready to create our dashboard for analysis.\r\nSteps\r\nDashboard Creation\r\nFrom the dashboard sheet, we drag and drop the ‘Fatalities by Countries and Event type’ and ‘No of Events in Country by Event Type’.\r\n\r\nNext, for the specific legends, we change it to floating type and drag it to the appropriate visualiastion it is used in.\r\n\r\nThe Dashboard is finally visualised as seen here. \r\n4. Observations from Makeover Visualisation \r\n From the above graph, Countries Indonesia,Philippines and Thailand had higher numbers of events compared to the remaining countries. Of note, there seems to be unrecorded data for Philippines before 2016, and for Malaysia, before 2018. Since the data is missing we are unable to make a fair compairison in terms of relative safety and security across the countries\r\n For a better comparison. I filter the years to start from 1st January 2016. From here, we can see the main bulk of Event types defer from country to country, Protests in Indonesia, Battles in Myanmar, Battles/Violence against Civilians/Protests in Philippines, Remote Violence and Protest in Thailand.\r\n Within this time frame the Philippines had the highest number of fatalities followed by Myanmar. This could due to Philippines president Duterte’s war on drugs. The fatalities in Myanmar were situatied in the Rohingya region.A northen region in myanmar. This was due to Tensions between Muslims and Buddhists communities\r\n Within Philippines the main actors of conflicts were the police forces and anti-drug vigilantes. This is in stark contrast to Thailand where the main actors were Malay muslim seperatists followed by protestors\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-21T19:55:04+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-dataviz-makeover3/",
    "title": "Dataviz Makeover3",
    "description": "Mapping armed conflict in South-east Asia in the last decade (Jan 2010 to Oct 2020)",
    "author": [
      {
        "name": "Atticus Foo",
        "url": "https://public.tableau.com/profile/atticusfoo#!/"
      }
    ],
    "date": "2021-03-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\n#1 Assessment of visualisation based on clarity, aesthetics and interactivity\r\n1.1 Clarity\r\n1.1.1 Ensuring Y-axis are consistent when comparing multiple charts of similar type\r\n1.1.2 Each event occurrence represented by a single dot\r\n1.1.3 Unable to tell when the event occurred on the map\r\n1.1.4 Unable to determine the intensity related to the event type\r\n\r\n1.2 Aesthetic\r\n1.2.1 Cohesiveness\r\n1.2.2 Clean-up to remove visual clutter.\r\n1.2.3 Proportion of visualisation\r\n\r\n1.3 Interactivity\r\n1.3.1 Filtering the relevant event types\r\n1.3.2 Filtering time or showing chronology of events\r\n1.3.3 Comparing Countries\r\n\r\n\r\n#2 Sketch of alternate design\r\n#3 Proposed Visualisation in Tableau\r\n3.1 Visualising Time:\r\n3.2 Visualising Space:\r\n\r\n#4 Step-by-step guide\r\n4.1 Data Preparation\r\n4.2 Preparing the Map\r\n4.3 Using Mapbox\r\n4.4 Using bounding box for an overview map\r\n4.5 using interactive animation for timeline\r\n4.6 time series circle chart\r\n4.7 Event selection\r\n4.8 Creating the Dashboard\r\n\r\n#5 Observations\r\n5.1 Conflict Trends in the region\r\n5.3 Differences between conflict types\r\n5.4 Difference between causes of fatalities\r\n5.5 Difference in Actors Involved\r\n\r\n#6 References\r\n\r\nAfter MakeoverOverview\r\nFor the purposes of this data-makeover, we will focus on mapping the geo-spatial patterns of armed conflict in South-east Asia countries between 2015 and 2020. Data is taken from The Armed Conflict Location & Event Data Project (ACLED) using the Data Export Tool provided. The tableau dashboard can be found here.\r\nBefore MakeoverThis blogpost will attempt to re-present (makeover) the interactive visualisation as seen in the screenshot above, applying techniques learned in Professor Kam’s class on geo-spatial visualisations. The following sections can be found below:\r\n• 1 Assessment of visualisation based on clarity, aesthetics and interactivity • 2 Sketch of alternate design • 3 Proposed visualisation in Tableau • 4 Step-by-step guide • 5 Insights from the visualisation\r\n#1 Assessment of visualisation based on clarity, aesthetics and interactivity\r\nWithout the context of the original communication intent of the above chart, it is difficult to provide a good assessment on how it can be improved. As such, the critique below will be based solely on the rubrics of clarity, aesthetics and interactivity.\r\n1.1 Clarity\r\nIn this segment, we will explore ways we can improve the visual clarity of this visualisation.\r\n1.1.1 Ensuring Y-axis are consistent when comparing multiple charts of similar type\r\n• The Y-axis on the chart on the right is not synchronized and may mislead. This is because they all count incident occurrence but they do not all start at zero and do not have the same ranges. This may lead (for instance) some readers to believe, at first glance, that the number of riots in 2018 was higher than the number of battles in the same year. As such, this affects the visual clarity of the charts when describing the number of “Armed Conflict Events by Type”.\r\n1.1.2 Each event occurrence represented by a single dot\r\n• While this method of visualizing each discrete incident is the most faithful representation of the data, it may not provide the most visual clarity. This is because many incidents can occur at the same location over time, and this would leave to the dots either overlapping on top of each other or being covered entirely. Placing the dots side-by-side may help if the density is not high, but is not an accurate representation of where the incident first occurred.\r\n• This can be using a heat-density plot / proportional symbol map or other similar plots in order to reflect volume of the event type in a certain geographic region. This can help readers to clearly assess if an event has tended to occur in the same location.\r\n1.1.3 Unable to tell when the event occurred on the map\r\n• The data contains events of armed conflict between 2015 to 2020, but readers are unable to tell when each event took place. As such, this does not allow for a temporal analysis unless a filter is added in.\r\n1.1.4 Unable to determine the intensity related to the event type\r\n• As this is a visualisation that aims to present armed conflicts that occurs across South-east Asia, more information can be presented (if available) in order to contextualize the event occurrence. This can related to information to fatalities, length of time of an event, or population density in the geographical region, and help readers better understand the possible reasons for a conflict in that area.\r\n• It is also important to the note that the date for 2020 is not complete – and should be indicated in the visualisation so that readers are clear that the dataset is not complete, least they get the wrong impression of the number of armed conflicts in 2020.\r\n1.2 Aesthetic\r\nUnder this segment, we will look at two main subtypes and the accompany areas that can be possibly improved.\r\n1.2.1 Cohesiveness\r\n• For greater visual cohesiveness, the line charts on the right can also be colour coded in a similar fashion to the chart on the left. This is because they represent the same event types. Readers will be able to quickly adapt to the information on the chart if the colour styles were consistently used.\r\n1.2.2 Clean-up to remove visual clutter.\r\n• Aesthetically, the graphic could have done with minor touch-ups to remove the clutter in order to improve aesthetics. For instance. The Y-Axis contains fields from the data-sheet, ‘Count of Sheet1’. This should be removed as it does not add anything to the Event Type which already describes the characteristics of the line chart.\r\n• Other minor ‘clean-ups’ can be done to ensure that the boundary of the selected country – in this instance, Myanmar, can be more clearly seen. This can be useful in assessing if the conflict events have taken place near a country’s border – hence signifying that the conflict has occurred across borders.\r\nAs there multiple bar charts stacked together, it may be beneficial including grid lines in order for viewers to track the datapoints along the x and y axis.\r\n• Lastly, the ‘Event Type’ legend can be improved as readers are unable to read the longer phrases in the legend. This may hamper their understanding of the visualisation – if they have no access to the codebook.\r\n1.2.3 Proportion of visualisation\r\n• At the moment, the map shares the same valuable horizontal real-estate as the time series charts. Visually, it would be more comfortable reading the map if it were allowed to be wider. The time-series charts can be moved down and will too benefit from a wider presentation as the x-axis would represent the different years across time.\r\n1.3 Interactivity\r\nIn terms of interactivity, more can have been done to take advantage of Tableau’s features to demonstrate geo-spatial patterns of armed conflict in South-east Asia countries.\r\n1.3.1 Filtering the relevant event types\r\n• The current chart does not allow you to filter by event types. This is important because the scale and intensity of each event type can differ greatly and it may not be useful to view everything at once. As such, it may be more useful to allow users to pick the different event types they are keen on exploring.\r\n1.3.2 Filtering time or showing chronology of events\r\n• As described above, the current presentation of events is across 2010 to 2020. This makes it difficult to understand if armed conflict had decrease or increased in a region. Interactive features can be in-built for readers to filter out events perhaps by year. This can also be improved if they were allowed to see the events in a chronological fashion – so as to understand the developments in the region / country across time.\r\n• For the above two sections, if would be best if the data in the chart on the right can be synchronized to the one on the left. This would help with visual clarity and consistency.\r\n1.3.3 Comparing Countries\r\n• Perhaps due to space constraints, it is difficult to see the visualization of more than one country. If it is possible to make country and country (within South-east Asia) comparisons, this may improve the value of the presentation and the use of South-east Asia data.\r\n#2 Sketch of alternate design\r\nSketchAs the main feature of the visualisation is the geospatial map of armed conflict across South-east asia, we wanted to give more real-estate to the map for the reader to be able to better visualize the events (of armed conflict) across the region. This is why the proposed sketch features the map heavily. As such, details the types of armed conflict and the volume is shifted below and contextualized in a way that aims to give you a good overview of the development not only across the region but also across time. \r\nIdeally, we will also integrate interactive tool-tips in order to save ‘space’. This will allow readers to find additional information when they mouse-over relevant information, such as the main actors involved in armed conflict in each region.\r\nLastly, as the treatment of the topic (of armed conflict in the region) is a serious one, we will be making use of the fatalities dataset (to show consequence), and using a somber monochromatic color theme with conflict and fatalities shown in various shades of red.\r\n#3 Proposed Visualisation in Tableau\r\nRe-presentedThe visualisation was built using Tableau Desktop Professional Edition 2020.40 and can be found here.\r\nThe key changes made in the data visualisation is outlined in the paper sketch in the above section. The main intent was to help the readers view events of armed conflict in the region across time and space, as well as to provide them useful contextual information so as to aid their understanding of developments in the past decade.\r\n3.1 Visualising Time:\r\nReaders are able to better understand where the incidents took place over space and time in the region as they are able to play an animation of the events through time. A time series chart is also located at the bottom, in order to provide useful information about the volume of armed conflicts over time. Interactive tooltips provide information on the actors involved as well as the detailed breakdown on the types of arms conflicts in the region.\r\n3.2 Visualising Space:\r\nAt the macro level, readers may be able to visualize the geographic location of the country in the region through the bounding box map in the top right. We have also adopted the use of a style map from Mapbox in order to show a more details on road networks and administrative buildings, in order to better understand if events have taken place due to geographical factors. Lastly, to better understand an event type, readers are able to click on the event to read up notes about the event.\r\n#4 Step-by-step guide\r\nThe following steps will provide an overview of how the visualisation was completed using the data extracted from ACLED and Tableau Desktop.\r\n4.1 Data Preparation\r\nConnecting ExcelThe data use for the visualisation was extracted from ACLED using their data export tool here. As seen in the screenshot below, you should be able to check the data fields using Tableaus data prep feature. We will also need to refer to the ACLED codebook in order to understand the features in the dataset. The ACLED codebook can be found here\r\nWe will now outline the few changes we made as part of data preparation. \r\nFirst, based on the codebook, the field Admin1 contains info on the state/provinces. It will be useful to create a hierarchy by dragging Admin 1 under the Country field in order create the hierarchy. You can also rename Admin1 to State / Province as we have for ease of reference later.\r\nLat LongNext, we will have to change Inter1 and Inter 2 from measures to dimensions. This is because while both fields contain integers 1 to 8, they are categorical data as they reflect actor types instead. As such, they can be changed to correctly reflect discrete categories.\r\nLastly we will want to ensure that longitudes and latitudes are not unnecessarily aggregated by changing them from Measures to Dimensions. (i.e. This is because a longitude of -5 is not less than a longitude of 5).\r\n4.2 Preparing the Map\r\nMappingWe can now start to prepare the map visualisation. First drag longitude to colums and latitude to rows. A map of South-east Asia should appear in the worksheet below. We can now drag Event Type to ‘colours’ in the Marks card and select ‘Circle’ in order to create a plot chart.\r\nScatterplotAt the same time, we want to be able to visually identify and filter out the different regions, time period as well as event types. To do this, drag the measures, Country, Event Type and Year to Filters. Right click on these measures in the Filters section and select ‘Show Filter’. You should be able to now select the types of filters you.\r\nSortingIf you want aggregated circle plots, so as not to overcrowd the map, you can also drag Event Types to Size in the Marks card. This will allow you to aggregate all the events that have taken place in the selected time frame. However, as some event types may overlap others (that have smaller values), you will need to sort the event types (as seen in the screenshot above) by ascending order in order for the smaller event types to be above larger event types on the map so that they can be visibly spotted.\r\ndual axisAs we want to also provide data of fatalities on the map, we will have to create a dual axis chart. We can do this by holding the ctrl button while clicking on the ‘Latitude’ measure in ‘Rows’ in order to duplicate it.\r\ninsert dual axisWhen this is done, you can drag the Fatalities measure to size and select density under the Marks card in order to change the fatalities sum to a density plot on the map.\r\nprocessing dual axisTo complete the map, right click on the Latitude measure in Row and select dual axis.\r\ncolorYou will also want to take this opportunity to change the colors of the Event Types and Density plot so that they are visible or match the colour scheme you have in mind.\r\n4.3 Using Mapbox\r\nmapboxFor this makeover, we wanted a monochromatic color scheme for the map, that still provided details on road networks. This was not an option within Tableau. As such, we utilized a third party app, Mapbox (mapbox.com) for this.\r\nmapbox studioTo do this, we will first need to create an account on MapBox. You will be able to create a free one at the link above. MapBox provides several different custom styles that are already available. In Mapbox studio, I am able to add the different details (such as transport nodes, administrative buildings, landcover etc.) to the map based on the style we prefer. We will also able to edit the colors and fonts of the icons and features on the map as see in the screenshot above.\r\nSharing mapbox stylesOnce done, click on ‘share’ to find a link you can use for integration with Tableau. Copy the API code so that you can use this in Tableau later.\r\nApplying Mapxbox StyleBack in Tableau, click on Map and then Background Maps and then Add Mapbox Map. As seen in the screenshot above, you can give the Mapbox a title. Paste the link you copied earlier in order to import the background Map from Mapbox Studio to your Tableau visualisation.\r\n4.4 Using bounding box for an overview map\r\nOverviewFor this visualisation, we wanted o create a tiny overview map which is highlighting the selected country with a ‘bounding box’. We relied on Klaus Schulte’s very helpful tutorial found here. The main intend behind the overview map is to give readers more context on where the selected country was situated in the region – as this could give clues as to the reasons for armed conflict in the region (e.g. whether the country was landlocked / or shared borders).\r\noverview2As seen the completed visualization, the overview map can give readers a better idea as to where Myanmar was located in South-east Asia. This is not easily possible by looking at the main map alone. The basic idea of a bounding box is to identify the ranges of latitudes and longitudes of the selected country so as to ‘bound’ the country’ in a rectangle. To do this, we will need to figure out the minimum and maximum longitude of the countries.\r\nExcelData was imported in excel and prepared there as seen in the screenshot above. Data was extracted from Github here https://gist.github.com/graydon/11198540#file-country-bounding-boxes-py.\r\nJoinThis excel was later imported into Tableau using Tableaus join feature. This can be done easily using the Country field.\r\nPathTo create the bounding boxes, simply drag longitude to columns and latitude to rows and point order to Path in the Line Marks card.\r\nPolygonThis will simply create only a rectangle that is not coloured. To fill the rectangle, create a dual axis chart by repeating the same method above. This time, however, select Polygon under the Marks card in order to create a rectangle that is filled. Select dual-axis in order to overlap the two charts.\r\nCountry ParameterTo complete the bounding box overview map, you will need to create a parameter “[Country]=[Country Parameter]” as seen in the screenshot above in other to filter the countries in the detailed map. Lastly edit the colors of both the polygon and rectangle border as you see fit. We picked red and set opacity to 30%.\r\n4.5 using interactive animation for timeline\r\nTimeTo create an interactive timeline, simply drag the measure Event Date to the Pages section. You can find this in the screenshot above. A filter will appear on the right, allow you to play the timeline of events chronologically.\r\nTimelineYou are also able to format the settings for this animation. To do this, select Format from the toolbar and then Animation. You should be able to then customize the settings for the timeline animation. We have set this to 0.5 seconds and for the style to be ‘Sequential’.\r\n4.6 time series circle chart\r\ntime seriesThis section is inspired by the work of Zunaira Rasheed. The intent of this chart is to demonstrate the total number of cases and fatalities in each country. This same data should also be presented across a span of time, in order for readers tell how much armed conflict has occurred in that country in the last decade. Tooltips will also provide more information on the type of events, the number of fatalities as well as the actors involved.\r\nBubbleThis time series is actually made up of three different circle charts as well as three different tooltips.\r\nFatalitiesFirst to create the Fatalities circle chart, we will drag the Country measure to Rows, as well as Country and Year to Filters. Next, add Fatalities as well as Records to the size section in Marks and Tooltip respectively.\r\nBordersIn order to create a clean looking chart when integrated, we removed the header as well as the borders as seen in the screenshot above. The colour for the circle was set to red with a black border and 80% opacity in the event the circles overlapped.\r\ntotal countThis was repeated for the country column, except the colours chosen was now grew.\r\nYear/fatalitiesAnd then again for the columns for the rest of the corresponding years. We will now have to create the corresponding tooltips for each of the sections.\r\ntooltipThe first tool tip is intended to convey the actors that was most involved in armed conflict in the region. To create this chart, drag Events as Aggregate measures to Colour and Size in the marks section. Also drag Country to the Filters section. Now, drag Actor1, Events and Year to the Label section. This will allow you to label the different part of the chart as seen in the screenshot above. Once again, change the colors as you see fit. We have chosen a red to match the consistency of the colour palette selected for this visualisastion.\r\ntooltip2The next tooltip, we want to chart all the events that have occurred in the country based on the filters. To do this, drag country to Filters. At the same time the measure Records to Columns and Event Type to Rows. We also changed the colours to gray and select ‘Always Mark’ for the labels in order to indicate the number of events at the end of the horizontal bar charts.\r\ntooltip3This is similarly repeated to show fatalities instead of total event count. As seen in the screenshot above, the measure Fatalities was selected instead of Records and the colour was changed to red.\r\ntoopltip4Finally we can start preparing the interactive tooltips. Click Tooltip under the Marks section in the Fatalities worksheet. You may use the formulation as seen in the screenshot above by selecting ‘Insert Sheets’. This will allow the chart you prepared earlier to appear as an interactive visualization when you mouseover this chart.\r\ntooltip5You may repeat this step for the Country Total worksheet with the tooltip formulation seen in the screenshot above. This will provide contextual information on the country selected, the total number of recorded events of armed conflict in the last decade as well as the sum total of fatalities involved.\r\ntooltip6Lastly, repeat this step for the Year worksheet. This time, instead of linking / inserting 1 worksheet, we will insert the two other worksheets we prepared. This will show bar charts of the number and type of armed conflicts in the year as well as the fatalities due to the event type in that year.\r\n4.7 Event selection\r\nselected eventFor the final part of the visualization, we will prepare a worksheet that would show the details of the event selected on the map. As seen in the map above, information about the user selected event, if any, will be taken from the Notes field of the data source. This section is aided by the work of Paul Wachtler\r\nevent descriptionTo do this, we will need to create one calculate field as seen in the screenshot above\r\nmost recent eventWe will then drag the measures Country and Most Recent Event Description (just created) to filters. We will also drag the measures Event Date, Event Type, State Province to the tooltip as Attributes. The new calculated field, Most Recent Event Description will be dragged to the Text section under the Marks card. \r\nAt the dashboard tab, click on Dashboard on the toolbar and then select Actions.\r\nEdit ActionYou can then select Add action in order to control the source and target sheets. Edit the settings so that they are similar to the screenshot above. This will allow readers to select an event on the map, and allow the target worksheet, ‘Most Recent Event’, draw the appropriate note for display.\r\n4.8 Creating the Dashboard\r\nCreating the DashboardFinally to plot the dashboard, select “New dashboard’ and set the resolution to custom size (1100 X 900). We rarely want to exceed 1100 because most computer screens in the market are only at Full HD resolution – meaning a width of 1080 pixels only. However, as the map is densely packed with data, we have picked 1100 for the width.\r\nsections linkedAn important step to make is to ensure that all the worksheets are accurately linked so that the charts reflect the data relating to the question selected. In this case, the time series charts the bottom should not be linked to anything else while the main map, the overview map and country selection and selected event should be linked together.\r\nworksheets usedThe worksheets used for the dashboards are as indicated in the screenshot above. Drag them onto the corresponding space on the dashboard – or as you see fit.\r\nfloatingOnly the legend for overviewmap should be floating.this is to allow it to appear integrated on the dashboard as a single value dropdown feature. Also deselect ‘All’ in the Customize section as this will greatly improve the speed and response of the dashboard.\r\ntextboxLastly add two textboxes using the feature in the bottom left. This will be used for both the dashboard title and ‘instructions’ as seen in the screenshot above.\r\nTitleYou may edit the textboxes by double clicking them like this. You can also use this feature to adjust the font size and colour. From here, cosmetic changes can also be made to the titles and legends by double clicking on them.\r\n#5 Observations\r\nBefore proceeding with the observations, we wanted to note that the data used for the visualisation is not complete. First the dataset is only up till October 2020 and not December 2020. Second, some countries such as Indonesia, Malaysia and Thailand have missing data before 2015. As such, our observations will be based on the limitations of this dataset.\r\n5.1 Conflict Trends in the region\r\nTrendsBased on the data collected, Phillippines and Myanmar observed the most number of armed conflict events in the last decade. In general, armed conflict in Cambodia and Thailand has trended downwards. The opposite is observed in Myanmar with approximately 6.1x more instances of armed conflict in 2020 than in 2010. This is compared to Cambodia which has about 2.3x less instances of armed conflict in the same period observed.\r\nIt is not possible to observe the same patterns in the Philippines as data is recorded only in the same year Rodrigo Duterte became president in 2016. However, in that short time, we observe that the number of armed conflicts has generally decreased from 3,194 in 2016 to 1,381 in 2020.\r\nThis is the reverse in Indonesia, where instances of armed conflict have increased from 599 in 2015 to 1,517 in after President Jokowi took charge as Indonesia President in 2014. Perhaps a better way to observe this trend would be to use a candlestick chart – so that we can observe the range of change better.\r\n##5.2 Patterns of conflict within Myanmar, Philippines and Thailand\r\nThailandIn the last decade, we observe that most of the armed conflicts have occurred in the southern region of Thailand with occasional clashes in the capital, Bangkok. South of Thailand shares the same border as Malaysia. Events of armed conflict that occur here are generally skirmishes (e.g. battles, explosions and violence against civilians) that lead to fatalities. This is different from Bangkok where the majority of incidents were protest and riots which rarely resulted in fatalities.\r\nPhilippinesA similar patten can be observed in the Philippines with conflicts occurring most in the capital Manila and the southern islands of Mindanao. The nature of the conflicts in Manila is slightly different from that in Bangkok, with the majority of the events occurring due to ‘Violence against civilians’ resulting in a higher proportion of fatalities.\r\nMyanmarIn Myanmar, armed conflict with casualties occur most consistently in the last decade in the Shan region. However, this changed in 2019 with a high concentration of incidents (‘Violence against civilians’) in the Rakhine region on the Western coast of Myanmar. This was often with fatalities.\r\n5.3 Differences between conflict types\r\nProtestsIn general, it appears that incidents in cities tended to be Protests and Riots. This could be due to the presence of highly developed road and transportation networks, which allow for large masses of people to easily congregate at. The screenshot above shows the number of protests and riots in Bangkok from 2016 to 2020.\r\nroadsThis contrasts with the Southern Thailand where the majority of events were battles. Based on the screenshot above, we can observe that road networks were not as developed or as concentrated than that of Bangkok. This relates to the similar patterns seen as well in the Philippines.\r\n5.4 Difference between causes of fatalities\r\nFatalities in PhilippinesWe are also able to observe the differences the causes of fatalities across these regions and are able to tell that the causes of fatalities are different. In the Philippines, the main causes of fatalities (4,088 in 2016) was due to ‘violence against civilians’. This is sub-categorized by ACLED as events of either sexual violence, attack or abduction.\r\nFatalities in MyanmarOn the other hand, fatalities (1,495 in 2018) in Myanmar resulted most from battles. This is due to armed clashes between different actors, government forces or non-state actors trying gain control over a territory. We are also able to observe that compared to the Philippines (59.7%), Myanmar (11.2%) witnessed proportionally less ‘Violence on civilians’ type of incidents.\r\n5.5 Difference in Actors Involved\r\nActors in MyanmarIn the past decade, the Military Forces of Myanmar (from 2011 to 2016 and 2016 to present) was involved in 52.32% of the incidents of conflict within the country. Protestors on the other hand, were involved 24.86% of the time.\r\nActors in PhilippinesThis contrasts with the Philippines where the majority of incidents involved the Police Forces of the Philippines (27.36%). Protestors were involved in a smaller percentage of incidents at 10.81%. Anti-Drug Vigilantes were involved in 18.74% of the incidents since 2016 and this has been criticized by HRW and Amnesty International https://www.amnesty.org.uk/philippines-president-duterte-war-on-drugs-thousands-killed as having been caused by Duterte’s ‘war n drugs’ campaign. This has led to approximately 7,000 killed between July 2016 and January 2017.\r\nActors in ThailandIn Southern Thailand, where skirmishes were most rife, the Malay Muslim Separatists of Thailand were involved in 65.39% of incidents in the past decade.\r\nAgain, we would caution that the observations made in this visualisation is confined to only publicly available data – and therefore has obvious limitations. Observations are also restricted mainly to the results of the quantitative data and does not include qualitative findings that can help to better understand the nature and context of conflicts in the region.\r\n#6 References\r\nThis data-makeover relied heavily on the makeovers found at Makeover Monday https://www.makeovermonday.co.uk/week-34-2018/\r\nThank you for reading up to this point!\r\nThis blog is a data visualization assignment for the MITB programme at the Singapore Management University.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-21T19:54:01+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-25-visual-analytics-project-proposal/",
    "title": "Visual Analytics Project Proposal",
    "description": "Understanding Key Stories Covered In the Media and How Readers Engaged With News",
    "author": [
      {
        "name": "Atticus Foo, Gerald Chee & Nam Jihun",
        "url": "https://public.tableau.com/profile/atticusfoo#!/"
      }
    ],
    "date": "2021-02-25",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Motivations and Objective\r\n2. Data\r\n3. Visualisation Features\r\n3.1 Topic Classification\r\n3.2 Time Series Analysis\r\n3.3 Anomaly Detection\r\n\r\n4. Challenges\r\n4.1 Classifying unstructured data\r\n4.2 Data cleaning for unstructured data\r\n\r\n5. Annex – proposed sites for analysis\r\n\r\n\r\n\r\n\r\nFigure 1: After Makeover.\r\n\r\n\r\n\r\n1. Motivations and Objective\r\nAs we become more and more inundated with news from various digital sources today, understanding what the key stories are across the digital spectrum is becoming more and more challenging. As such, we are interested in understanding how to best present a visual snapshot of the key stories that are covered in local media and identifying how readers engaged with the news.\r\n2. Data\r\nTo do this, we will be analysing English news articles published by 9 local news sites for the year of 2020. To identify how readers engaged with each article, we will be using Facebook engagement data (Likes, Shares and Comments) retrieved from CrowdTangle and Prrt. This amounts to 94,841 news articles and its corresponding Facebook engagement data.\r\n3. Visualisation Features\r\nWe plan to feature three visualisation modules: - Topic Classification in order to understand key news topics - Time series analysis in order to identify current trends and forecast future ones - Anomaly detection in order to detect key news items\r\n3.1 Topic Classification\r\nTopic ClassificationTopic classification could be the most challenging as it would require text analytics. This is due both to our unfamiliarity with text analytics as well as the localised nature of our news headlines – even if it is in the English language.\r\n3.2 Time Series Analysis\r\nTime Series AnalysisAs we will be looking at an entire year’s worth of data, we should be able to statistically quantify past trends. Using this, we should also be able to forecast future trends, such as predicting the most popular days for news engagement.\r\n3.3 Anomaly Detection\r\nAnomaly DetectionAs it is important to quickly sift through all the noise to identify news trends, we propose building an anomaly detection visualisation that could present articles that have gone viral. This can be done by identifying if there are engagement rates that have fallen outside the norm.\r\n4. Challenges\r\nAt this current stage, we foresee two main challenges for this project. They are:\r\n4.1 Classifying unstructured data\r\nMuch of the project relies on being able classify article headlines for further statistical analysis. As identified above, this is our biggest challenge. Dealing with unstructured data will be difficult especially since the team is unfamiliar with text analytics.\r\nThis will be made tougher as the localised nature of news headlines from local media will mean that we will have less libraries and resources to call upon online.\r\n4.2 Data cleaning for unstructured data\r\nThe unstructured nature of the data will also present a second challenge for data cleaning. As much of the data is taken from public scrappers, and Facebook metadata, we have to ensure that there are no duplicates in values as well as odd text in the dataset.\r\n5. Annex – proposed sites for analysis\r\nAsia One\r\nChannel News Asia\r\nMothership\r\nMust Share News\r\nRice Media\r\nStraits Times\r\nThe Online Citizen\r\nToday Online\r\nYahoo SG\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T14:53:19+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-19-dataviz-makeover-2/",
    "title": "Dataviz Makeover 2",
    "description": "Re-imagining YouGov's survey on attitudes and opinions towards COVID-19 vaccination.",
    "author": [
      {
        "name": "Atticus Foo",
        "url": "https://public.tableau.com/profile/atticusfoo#!/"
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\n#1 Assessment of visualisation based on clarity and aesthetics\r\n1.1 Clarity\r\n1.1.1 Titles and legends have not been edited\r\n1.1.2 Visualising uncertainty\r\n1.1.3 Juxtaposition of different types of bar charts\r\n1.1.4 Axis values not sorted\r\n\r\n1.2 Aesthetics\r\n1.2.1 Use of colours\r\n1.2.2 Legends\r\n1.2.3 Gridlines\r\n\r\n\r\n#2 Sketch of alternate design\r\n#3 PROPOSED VISUALISATION IN TABLEAU\r\n#4 Step-by-step guide\r\nData Preparation\r\nPlotting the Diverging Bar Chart with Neutrals Aside\r\nCreating the Error Bar on Dot Plot\r\nCreating the Waffle Chart\r\nCreating the Dashboard\r\n\r\n#5 Analysis\r\n5.1 Asian Countries, South Korea, Singapore and Japan less pro-COVID-19 vaccine?\r\n5.2 Worried about the potential side-effects of the vaccine.\r\n5.3 Older Singaporeand who are unemployed may be at risk\r\n5.4 Low Response Rates May Affect Results\r\n\r\n#References:\r\n\r\n\r\n\r\n\r\nFigure 1: After Makeover.\r\n\r\n\r\n\r\nOverview\r\nAs COVID-19 vaccines roll-out across the globe, the actual vaccination rates can vary due to various reasons including political ones. For the purposes of this data-makeover, we will focus on the opinions and attitudes of respondents towards COVID-19 and the vaccination around the world. Data is taken from the Imperial College London YouGov Covid 19 Behaviour Tracker Data Hub here.\r\nThis post will attempt to re-present (makeover) the chart found in the next section, applying techniques learned in Professor Kam’s class on visualising uncertainty, as well as Steve Wexler’s extensive thoughts on presenting ‘neutral’ values. The following sections can be found below:\r\n#1 Assessment of visualisation based on clarity and aesthetics\r\n#2 Sketch of alternate design\r\n#3 Proposed visualisation in Tableau\r\n#4 Step-by-step guide\r\n#5 Insights from the visualisation\r\n#1 Assessment of visualisation based on clarity and aesthetics\r\nSource: BeforeWithout the context of the primary communication intent of the above chart, it is difficult to provide an assessment of how it can be improved. As such,the critique below will be based solely on the rubrics of clarity and aesthetics.\r\n1.1 Clarity\r\n1.1.1 Titles and legends have not been edited\r\n• The current titles are not the most intuitive – as the corresponding charts do not convey the intended message. For instance, the first chart is labelled ‘which country is more pro-vaccine’, but the corresponding chart is a representation of how respondents in respective countries voted on a scale of 1 to 5. As such, it is not very clear how the first chart conveys the intended message, as the chart on the right would suffice. There is also a small typographical error in the title of the first chart.\r\n• The subtitles of both charts % of Total Record and % Strongly Agreed, should be edited.\r\n1.1.2 Visualising uncertainty\r\n• As data in the charts are represented as percentages, it is not possible to get a sense of the proportion of respondents from each country. As such, readers would assume that the values presented in the charts are equal and consistently reproducible to the same degree. Survey data can be impacted by various factors such as the number of respondents involved (thus affecting Confidence Intervals).\r\n1.1.3 Juxtaposition of different types of bar charts\r\n• The use of a 100% stacked bar chart on the left helps readers to easily compare values easily as they have a common baseline in 0% and 100%. However, in terms of clarity, the use of different bar charts (stacked bar chart on the left and horizontal bar chart on the right) may detract as the same value will appear visually different. This is not helped by the different treatment of the X-axis in both charts, with the first chart using 0 to 100% and the second chart ending at 60%. As a result, the value of ‘strongly agree’ on the left would look different from the one on the right even though they are of the same value.\r\n1.1.4 Axis values not sorted\r\n• The Y-axis of the chart on the left is sorted alphabetically while the chart on the right is sorted in descending order. As the categories in the y-axis of both charts are the same (countries), they should be sorted for visual clarity and allow readers to more easily reference both charts at once. This can be sorted by volume of those who are more agreeable to vaccination – to reflect the crux of message found in the chart title.\r\n• Additionally, as both charts will have the same Y-axis, the country labels on the second chart can be removed to reduce visual clutter.\r\n1.2 Aesthetics\r\n1.2.1 Use of colours\r\n• The choice of colours is good as it is contrasty and is generally able to help readers discern the differences between the discrete categories found in the chart. Further improvements could be made if the choice of colours were more deliberate. For instance, the current selection of red to represent value 3 (neutral) could be replaced with grey, a colour that is more universally accepted as neutral.\r\n• This is similar for the selection of green which is generally associated with ‘agree’ or ‘positive’ rather than its current representation which is ‘strongly disagree’. This is repeated in the choice of orange, which is placed beside red/neutral, but actually reflects ‘agree’.\r\n• As the categories selected are on a Likert scale, dual colours with variances could have been employed in order to highlight the differences in choices along the scale.\r\n1.2.2 Legends\r\n• In terms of placement, the colour legend on the right could be placed nearer to the chart on the left for easier visual reference. It also assumes that readers have prior knowledge as to what the categorical values 2, 3 and 4 mean as they are currently unlabelled. It is also unclear what ‘Vac 1’ means.\r\n• In terms of order, the order of the colours for legend could be reversed so that it matches the chart on the left where the colours go from green to blue, instead of the current order which is blue to green.\r\n1.2.3 Gridlines\r\n• The use of gridlines in the second chart help to negate the use of value labels which would add to clutter and distract from the overall presentation of the chat. This can be improved by using narrow bands for the tick-marks as the current 20% increment is quite wide, making it difficult to visualise accurately. This is not helped by the fact that most of the values fall between 20% to 45%.\r\n#2 Sketch of alternate design\r\nSource: Sketch of alternativeThe sketch above shows a diverging bar chart with the neutrals placed to the side. This is because we want to give stronger emphasis to the stronger views by placing them in the center of the chart.\r\n When the neutrals were placed in the center of the chart, it makes it difficult for the reader to compare the strongest values taken from the survey.\r\nSource: Neutrals SideInitially, we wanted to place the neutrals on the outside but this would mean artificially splitting the neutrals. This was not ideal as the neutral value was neither positive nor negative. As such, we would attempt to place the data to the side.\r\nIn terms of colours, we re-arranged the values according to colour temperature as well as contrast. We will also sort the countries (Y-Axis) so that both axis are consistent for easier visual referencing. In terms of visualising uncertainty, we want to add a confidence interval, to suggest the range of values that the respondents of the respective country were most likely to select. This would be marked at the 95% interval.\r\nSource: TooltipAs there is a lot useful demographic data collected by You Gov, we wanted to see if we could apply this to the dashboard in the form of an interactive tool-tip – and to see if any discernible findings could be made.\r\n#3 PROPOSED VISUALISATION IN TABLEAU\r\n\r\n\r\n\r\nFigure 2: Data Re-presented.\r\n\r\n\r\n\r\nThis visualisation was built using Tableau Desktop Professional Edition 2020.40 and can be found here\r\nThe key changes made in the data visualisation is explained and outlined in the paper sketch above. The Likert Scale used in the YouGov survey has been reproduced in a diverging stacked bar chart with the neutral values at the side.\r\nInstead of a bar chart on the right, we have adopted the line and dot chart in order to present the range of values that more likely to occur for the response ‘Strongly Agree’. More granularity is also added – so as to present differences between male and female responses (if any).\r\nSource: Waffle ChartFor the tool tip, we experimented with the waffle chart (since it was more space efficient) to visualise demographic data (age groups and employment status). However, this implementation was not ideal and can be improved in further iterations. Finally A series of five questions can be selected with the data applying to both charts at once.\r\n#4 Step-by-step guide\r\nThe following steps will provide a brief overview of how the visualisation was completed.\r\nData Preparation\r\nThe data used for the visualisations were taken from the Imperial College London YouGov Covid 19 Behaviour Tracker Data Hub here.\r\nSource: Data UnionAs the data is separated in different countries, the different csv files have to be extracted. Using Tableau’s union function, we be able to join the files together.\r\nWe will need to hide unnecessary fields – such as demographic data or other health related questions. This is because we want to focus specifically on questions related to vaccination. We can do this by clicking “Manage Metadata” and then hiding the selected fields.\r\nThe fields we want to retain are country, age, gender, household size, number of children in household, employment status and questions related to the vaccine (vac2_1, vac2_2, vac2_3, vac2_6 and vac3).\r\nSource: Data PivotAfter checking the field names using Tableau’s Data Prep to ensure that they are accurately categorised (e.g. numeric and string), we want to select all the question fields and pivot the values as seen in the photo above.\r\nNext we want create additional fields for QuestionID as well as Answers and Scores The latter two categories are to create string and integer values for the responses. The screenshots including the codes can be found in the photos below.\r\n\r\nPlotting the Diverging Bar Chart with Neutrals Aside\r\nWith the data prepared, we can begin plotting the first chart – which is the diverging bar chart. To do this, we will need to prepare the data by sorting the responses (strongly disagree, disagree, neutral, agree, strongly agree) into proportions (percentages). To do this, we will first create a ‘dummy’ field called Number of Records Ref before we can start preparing the calculated fields.\r\nSource: CF PositiveAs seen in the photo above, the field [Positive %] is defined as: SUM(IF [Score]>=4 then 1, else 0 END)/ SUM([Number of Records Ref]). This calculates the sum of both agree and strongly agree responses divided by the total number of responses. Repeat this to get the percentage for negative as well as neutral (neutral = 3) responses as seen in the screenshot below.\r\nSource: CF NegativeIn order to show the 4 levels of sentiment, we will need to need to create another calculated field [POSNEG]. The exclude statement ensures that Tableau doesn’t double count each of the 5 levels of sentiment.\r\nSource: CF POSNEGTo plot this chart, we will need to place the calculated field [POSNEG] and [Neutral %] into the columns section and [Question] and [Country] into rows as seen in the screenshot below. This will create a dual axis chart that will keep the neutral values separate.\r\nSource: Dual AxisWe will need to filter by [Answers] so as to remove all the null values – these are incomplete responses and hence should be removed from the findings as they would affect the percentages. At the same time, we will add [QUESTION] to the filter and select ‘show’. This will allow you to select the difference questions on the right – and the chart will respond dynamically to the selections. You can edit the appearance of the filter by choosing a drop down menu or a multi-select option.\r\nSource: Exclude FilterIn order to sort the countries by descending order to give a better visual representation, right click [Country] and select sort ‘Field’. Choose descending order, the appropriate field name (in your case [Positive %] and aggregation by ‘Average’.\r\nSource: SortSource: ColoursLastly to complete this chart, we would want to improve the colour scheme by clicking on the ‘Colour’ icon. This will allow you to adjust the colours of the values. We have gone with the colour scheme seen in the screenshot below to give a more association towards similar values and darker colours to represent stronger opinions (agree or disagree).\r\nCreating the Error Bar on Dot Plot\r\nSource: Error PlotNext to create the error bar on dot plot for ‘Strongly Agree’ responses, we would have to create a few calculated fields in order to identify the lower and upper limit for confidence intervals at 95 and 99%. We can first do this by creating calculated fields for Z-scores of 95 and 99 as seen in the screen shot below.\r\nSource: ZScoresTo calculate the proportion of ‘Strongly agree’ responses, we will create a calculate field [C – Strongly Agree] using the formula “IF [Score]=5 THEN 1, ELSE 0”.\r\nUsing this we can identify the percentage proportion by using the next calculated field formulate “SUM([C - Strong Agree ])/SUM({Exclude [Answer]: SUM([Number of Records Ref])})”. Next we can create the [C – Prop_SE], [C-Prop Margin 99] and [C – Prop Margin 95] using the calculated fields seen in the screenshot below to identify lower and upper limit.\r\nSource: Prop SEThe final 2 calculated fields we want to create for this step is the lower and upper bounds of the 95 confidence interval – so that we can create a line across the bubble plot to demonstrate the interval. We can do this by simply using the codes “[C - Prop] - [C - Prop Margin 95]” and “[C - Prop] + [C - Prop Margin 95].\r\nSource: Plot1To plot the error bar, we will need to create a dual axis chart. We do this by moving the [C-Prop], [C-upper] and [C-lower] fields create earlier into the columns section and [Question] into rows.\r\nSimilar to the steps for the diverging bar chart, we will want to filter by [Question] and [Answer]. The dual axis chart will look similar to the screenshot above.\r\nSource: LineIn order to change the lower and upper limits into a line bar, we can click on “Measure Values” and select the line chart option. We will also need to click on “Path” and select the third option for “Line Type” in order to obtain the line bar as seen in the photo above.\r\nSource: Dual AxisNext, right click on the axis on the second chart and select ‘Dual Axis’ in order to combine both charts. Effectively, these collapses both charts and overlays both the dot plot and line bar on top of each other.\r\nSource: Synchronizing the axisRight click on the same x-axis again and select synchronise axis in order to ensure that the charts are not distorted due to a difference in dimensions on the axis. The error bar on dot plot will not be complete.\r\nCreating the Waffle Chart\r\nSource: Waffle ChartThe additional waffle chart created for this visualisation is intended to provided additional demographic data in the tool tip – and serve as an alternative to to the typical bar chart which takes up more space as seen below.\r\nSource: Bar ChartIn order to crate the waffle chart, we will first need to create a 10x10 template in an excel file. It should look like the screenshot below. Add this new data source. Once it is added, convert “Column” and “Row” to “Dimension” by right clicking on each.\r\nSource: waffle TemplateNext we will have to build the skeleton of the waffle chart by dragging the dimensions “Column” to column and “Rows” to rows. Select the “Square” chart in mark type and adjust the size by using the slider and dragging the bottom and right axis to resize.\r\nSource: Waffle SkelettonNext we need to prepare calculated fields for all the age groups. To do this we need to bin the different age groups by using the formula in the screenshot below. Repeat this for the other age groups (depending on the range you prefer).\r\nSource: Age GroupsWe can now create a calculated field to calculate how much each proportion of the age group [A – AGER] is using the formula in the screen shot below.\r\nSource: FormulaTo plot the chart, drag [A – AGER] to “Colour”, “Country”, “Employment Status” and “Question” to Column and “Answer” to Row. Similar to previous steps, use the filter option on “Answer” and “Question” in order to remove null values and reflect the desired data relating to the target question. Edit the colours of the Waffle Chart using the option found in Marks. We have opted to use a greyscale to reflect the different age groups.\r\nCreating the Dashboard\r\nSource: DashboardFinally, to plot the dashboard, select “New Dashboard” and set resolution to Custom Size (1000 x 1000). We will be adding three worksheets (Diverging Bar Chart, Error Bar and Waffle) to the dashboard.\r\nSource: WorksheetsAn important step to make is to ensure that all the worksheets are linked so that all charts reflect the data relating to the question selected. This is important because the results used in the second chart is derived from the first. To do this, right click on the filter and apply the data sets to the selected worksheets as seen in the screenshot below.\r\nSource: TooltipWe will also need to add a tooltip in order for the waffled chart to be displayed when the reader hovers over the Diverging Bar Chart. To do this, navigate back to the first worksheet and click on the Tooltip selection in Marks. Insert the worksheet which the Waffle Chart is named in order to create the tooltip. \r\nLastly, layout the charts onto the dashboard as seen the screenshot above. We have chosen put both charts on top of each other instead of side-by-side due to the wider aspect of dual-axis charts. From here, cosmetic changes can be made to the titles and axis titles / legends by double-clicking on them.\r\n#5 Analysis\r\n5.1 Asian Countries, South Korea, Singapore and Japan less pro-COVID-19 vaccine?\r\nSource: Pro Vaccine?Asian countries like South Korea, Singapore and Japan appeared to be less pro vaccine, with 34, 29 and 25% of their respondents stating that they would strongly agree to taking the vaccine within a year from now, if it becomes available to them.\r\nWhile they felt less strongly towards the vaccine, they were also less likely to disagree with the vaccine (Singapore’s 8% strongly disagreeing compared to Germany’s 23%). At the same time, there is a greater proportion of neutral respondents from these countries, suggesting that they could be persuaded if circumstances change or if there was a greater public communications drive to do so. Comparatively, countries with high number of cases and death rates have greater responses.\r\n5.2 Worried about the potential side-effects of the vaccine.\r\nSource: Potential Side EffectsRespondents France, Spain, Japan and Singapore were the top countries in terms of those who were most worried about the potential side effects of the vaccine.\r\nSource: GovThis is interesting as more than 60% of Singaporeans agreed that the Government would provide them with an effective COVID-19 vaccine (with less than 10% disagreeing otherwise).\r\nAdditionally, female respondents tended to respond strongly to agree that they were worried about than male respondents the potential side effects of the vaccine. This is tracked at the 95% CI limits as seen in the chart above.\r\n5.3 Older Singaporeand who are unemployed may be at risk\r\nSource: WaffleBased on the waffle chart, the proportion of older Singaporeans in Singapore who were unemployed were more likely to strongly disagree that the Government would provide them with an effective COVID-19 vaccine.\r\nSource: Waffle2They were also more likely to strongly agree about the side effects of the COVID-19 vaccine and they strongly disagreed with getting vaccinated if it were available to them within a year.\r\nSource: Waffle3Conversely the proportion of respondents who were younger Singaporeans who were also unemployed were more worried about getting COVID-19.\r\n5.4 Low Response Rates May Affect Results\r\nSource: Low Response RatesFor the survey question, I am worried about getting COVID-19, Israel’s results are strongly pro-vaccine, with about 55% of respondents stating that they intended to get the vaccine within a year if it were avaialble to them. This puts them slightly behind the UK in terms of rankings.\r\nHowever, due to the CI plot seen on the dot on line bar plot, we can identify that there is a very wide CI. This is because the number of respondents are low.\r\n#References:\r\nRethinking the divergent stacked bar chart – placing the stronger views in the centre Tableau Playbook – Waffle Chart\r\nThank you for reading up to this point!\r\nThis blog is a data visualisation assignment for the MITB programme at the Singapore Management University.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-03-19T14:51:25+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-19-datavizmakeover2/",
    "title": "Dataviz Makeover 2",
    "description": "Visualisation Makeover on public willingess to take Covid-19 Vaccine in Tableau",
    "author": [
      {
        "name": "Chee Kah Wen Gerald",
        "url": "https://public.tableau.com/profile/gerald.chee1204#!/vizhome/DataVizMakeover2_16137273661220/Dashboard"
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Introduction\r\n2. Critque on Clarity and Aesthetics\r\n2.1 Proposed Design\r\n\r\n3. Preparation \r\n3.1 World Map \r\n3.2 Likert Scale \r\n3.3 Error Bar on Dot plot \r\n3.4 Dashboard \r\n\r\n4. Observations from Makeover Visualisation \r\n\r\n1. Introduction\r\nIn 2020, British international Internet-based market research and data analytics firm, YouGov partnered with the Institute of Global Health Innovation (IGHI) at Imperial College London to gather insights on people’s behaviours in response to COVID-19. To date,their research has covered 30 countries. From the publicly available data, A research team is currently conducting a study to understand the willingness of the public on Covid-19 vaccination.\r\nFIGURE 1: VISUALISATION DONE BY RESEARCH SCIENTISTBased on the above visualisations, the task is to make a critque on the Clarity and Aesthetics of the charts, providing an alternative visualisation and listing down the steps to ensure repoducibility.\r\n2. Critque on Clarity and Aesthetics\r\nClarity\r\nAesthetics\r\nNo clear distinction that the chart on the right is a subset of the left. Difficult to visualise the number of responses because this will have an impact on the reproducibility of this result in the survey.\r\nBlue Colour used for both charts to might be confusing to the reader.\r\nY-axis of both charts do not match due to different sort order. This confuses the reader as they are placed side by side.\r\nColor scheme is confusing. Red is mainly used to denote negative performance values, while blue or green is used to denote positive performance values.\r\nStacked bar chart shows proportion of response within countries well, however since the size of each bar is not properly aligned, it is difficult to compare down the table(between countries).\r\nColor legend placed next to horizontal bar that is completely blue in color. May mislead people as reader might expect the colors to be visually represented in that graph when it actually denotes the responses in the left chart.\r\nX-axis of both charts are not standardised. Chart on left has no decimal places and ends at 100% while chart on right has axis labels in 1 decimal place and ends at 60.0%, this is misleading to the reader as they may percieve the importance of the message the chart is trying to convey.\r\nColor Legend is not properly labeled, the reader is unable to interpret what the values of 2, 3 and 4 represent.\r\nLack of labels on left chart to show exact proportion or count of responses, reader would need to infer from the X-axis and the part-to-whole relationship.\r\nSweden,Canada,Norway,Finland have similar percentage values that responded ‘Strongly Agree’. This is hard for the reader to determine the eaxct proportion values as the x-axis interval increases by 20%. A smaller interval and gridline would help to make this information more distinct.\r\n2.1 Proposed Design\r\nPROPOSED VISUALISATION IN TABLEAUA likert scale chart is used to represent the diverging responses. This more clearly represents the proportions and visual scale of the differing opinions of different countries.\r\nThe colour selection: red tones for negative responses, blue tones for positive responses and grey for neutral response.To prevent looking like a ‘traffic light’ as compared to prior visualisation. With a proper colour legend.\r\nProperly spelled titles to give readers a sense of confidence in the researcher who prepared the presentation.\r\nParameter added to control the change of category, allowing readers to view data based on specific category.\r\nFilters added, giving the reader a choice on what data he/she would like to exclude for analysis.\r\n3. Preparation \r\nThe original source of the data can be found at Imperial College London YouGov Covid 19 Behaviour Tracker Data Hub\r\nSteps\r\nData Preparation\r\nAfter downloading covid-19-tracker-master, check if contents of folder tallies with description found on github\r\n\r\nWithin data folder contains zipped folders by country. Extract all folders and placed individual csv files into covid-19-tracker-master/data\r\n\r\nAfter opening Tableau,drag and drop any of the csv files (I used data from ‘Israel’ due to its small file size for quicker load times). Create a union with the current data source. Using the union by Wildcard, the ‘include’ field is left blank. This option creates a union of the current data source with all files located in the same folder\r\n\r\nAfter the union, a column called ‘path’ is created which is populated by the file name the data originated from.Aliases were created to change the displayed values to its Country name.\r\n\r\nSince not all columns are useful for analysis, click on manage metadata button to display column headers in a list view. Selected columns unnecessary for analysis will be hidden\r\n\r\nAfter the above steps, the cleaned dataset was extracted and renamed ‘Cleaned’ before reconnecting to Tableau\r\n\r\nFor Columns Vac2_1,Vac2_2,Vac2_3,Vac2_6,Vac 3, their columns were renamed according to the values found in ‘codebook.xlsx’\r\n\r\nBecause the age bands would be required for visualising the age-sex Pyramid, values were grouped together in bands of 5 years\r\n\r\nColumns were created by pivoting the selected columns Vac2_1,Vac2_2,Vac2_3,Vac2_6,Vac 3(already renamed) into ‘pivot field name’ and ‘pivot field value’. They were renamed ‘Question’ and ‘Score’ for visualisation\r\n\r\nAliases were created for ‘Score’ to display interger values\r\n\r\nI also grouped the employment status to 3 different groups namely, Available income source, No income source and NA (Null and other responses were placed here as it is unclear if they responded or the value of other)\r\n\r\nAfter the above steps, the ‘Score’ column was filtered to remove the ‘null’ values as we would not be able to derive any insights for analysis.The final data set was exported for use to prepare the visualisations\r\n\r\n3.1 World Map \r\nSteps\r\nVisualisation for World Map\r\nFor the world map,This column is renamed ‘Country’ and assigned geographic role as Country/Region.\r\n\r\nLongitude and Latitude calculated fields are auto generated in Tableau due to their assigned geographic role. Together with the count of row values in the csv. They will be used to visualise the map.\r\n\r\nLatitude values are placed in Column and Longitude Values placed in Row. From the ‘Show Me’ tab, select the only available map visualisation\r\n\r\nTo populate the map with color and labels, the count of all records are added. The country field is added into the detail button to visualise the map shapes.The legend display is display the Color to represent the number of respondants\r\n\r\nThe title of the legend is corrected to display ‘Number of Participants’. Since the colours do no start from 0 due to number of participants, the color is edited to start the color gradient from 0.The color of choice would be orange so as not to conflict with the blue that will be used in the likert scale\r\n\r\nThe World Map is finally visualised as seen here. \r\n3.2 Likert Scale \r\nBased on the task requirements, we need select how we would like to display the data based on the demographic category.Thus we would need select parameters for the visualisation.\r\nSteps\r\nParameter Creation for breaking down information by Category\r\nFrom a new sheet, we create a parameter called ‘Select Category’ using ‘create Parameter’ function. By using the formula as shown on the right. This would help map the ‘Score’ values to the associated Sentiment.\r\n\r\nBased on the selected parameter, we need to create a calculated field for each case within the ‘Select Category’ field.\r\n\r\nWe are ready to make a likert scale to easily visualise the results of the survey. To evaluate the proportion of respondents easily, we can divide axes and compare them across positive and negative responses.\r\nSteps\r\nVisualisation for Likert Scale\r\nWe create ‘Response’ field using ‘create calculated field’ function. By using the formula as shown on the right. This would help map the ‘Score’ values to the associated Sentiment.\r\n\r\nNext, we create ‘No of Records (Ref)’ field using ‘created calculated field’ function. This would be a reference field to for future formulas and references.\r\n\r\n‘Total count’ field is created to evaluate the total count. Note that this field is evaluated using the ‘Response’ field created in step 1 of this table.\r\n\r\n‘Count of Negative’ field is created to evaluate the negative scores which are 4 and 5.\r\n\r\nCreate ‘Total Count of Negative’ field using “created calculated field” function. Note that this field is evaluated using the ‘Response’ field.\r\n\r\nCreate ‘Gantt Start’ field using ‘created calculated field’ function.\r\n\r\nCreate ‘Percentage’ field using ‘created calculated field’ function to visualise the size of the bar within the likert scale.\r\n\r\nCreate ‘Gantt Percent’ field.Note that this field is evaluated using the ‘Response’ field.\r\n\r\nDrag and drop ‘Gantt Percent’ field into Columns, ’Country’ and ‘Country’ ’Category Selection’ into Rows. Drag and drop ‘Response’ into the color mark, ‘Percentage into ‘Size’ Mark. Change the display of the visualisation to ‘Gantt Bar’.\r\n\r\nFrom the ‘Response’ in the color mark, we do manual sorting of the response. From our axis, the left side denotes negative responses, thus it will be sorted according to the order shown in the screenshot.\r\n\r\nFrom the ‘Response’ color legend in the color mark, we manual chose the color scheme to represent the responses. I chose red tones for negative responses, blue tones for positive responses and grey for neutral response.\r\n\r\nDrag and drop ‘Percentage’ field into the Label button to display the percentages of the gantt bar.\r\n\r\nDrag and drop fields ‘Country’,’Gender’,‘Question’,’Employment Status (group)’,’Category Selection’ and ‘Age (group) into ‘Filter’ pane. These filters are applied through all worksheets using this data source for linking the different sheets together.\r\n\r\nThe Likert scale is finally visualised as seen here. \r\n3.3 Error Bar on Dot plot \r\nVisualising uncertainty\r\nIf the same survey with the same questions and responses options was conducted at a different time frame, are we likely to see the same proportion of respondents from each country? we are unable to tell based on the likert scale alone. As such, we would need to set a confidence interval to find out whether 95% of the time, the participants who responded with ‘Strongly Agree’ fall within the mean.\r\nThus we shall visualise the response data using a Error bar on dot plot\r\nSteps\r\nVisualisation for Error Bar on Dot plot\r\nFirst we create calculated field labeled ‘Proportion that Strongly Agree’.\r\n\r\nNext we create calculated field labeled ‘Proportion (Pro-Vaccine)’.\r\n\r\nNext we create calculated field labeled ‘Proportion_SE (Pro-Vaccine)’.\r\n\r\nSince we are using a 95% confidence interval, the Z-value is placed into a calculated field labeled ‘Z_95%’.\r\n\r\nNext we create calculated field labeled ‘Prop Margin of Error 95%’.\r\n\r\nFinally, we create the Upper and lower limits of error.\r\n\r\nWe drag and drop ‘Measure Values’ and ‘Proportion (Pro-Vaccine)’ into column and ‘Country’ into rows.\r\n\r\nFrom ‘Measure Values’ card, we remove all other field except upper and lower limits.\r\n\r\nSince the Columns have 2 field values, we place them in the same graph using dual axis and syncrhonise them\r\n\r\nIn the ‘Measure Values’ mark, we change the option to display as a line.Then, we drag measure names into the path button.Next We filter out all other fields except Prop_Upper limit 95% and Prop_Lower Limit 95%\r\n\r\nIn the ‘Proportion (Pro-Vaccine)’ mark, we change the display to display as a circle and attach ‘Measure Name’ to color so we can change the plot and its color to not clash with the likert scale.\r\n\r\nWe Next add the ‘Category Selection’ Field into Rows.\r\n\r\nThe Error Bar on Dot plot is finally visualised as seen here. \r\n3.4 Dashboard \r\nWe are now ready to create our dashboard for analysis.\r\nSteps\r\nDashboard Creation\r\nWithin, the sheets used for the dashboard, we display all filters and parameters used. Since they are linked to multiple sheets. It will help in our analysis of the data set.\r\n\r\nIn the Dashboard sheet, we drag and drop the likert scale and error bar on dot plot.\r\n\r\nNext we rearrange the filters and parameters to be displayed at the top.\r\n\r\nThe Dashboard is finally visualised as seen here. \r\n4. Observations from Makeover Visualisation \r\n From the above graph, Countries Japan, South Korea, Spain and Singapore had higher percentage of respondents who ‘agree’ or ‘Strongly agreed’ to to being worried about getting the Virus. Where as the remaining countries seem to be ‘split 50-50’ on their opinion.\r\n France and Germany are shown to have high proportion (19.26% and 22.97%) of strongly disagree to taking the vaccine if it was offered in a years time.\r\n UK and Denmark are shown to have higher proportion (48.44% and 50.20%) of respondants who would regret not getting the vaccine. On the other end France has the highest proportion who strongly disagree to this\r\n Within all Countries, there is a higher proprotion of females compared to males who strongly agree that there are potential side effects to the vaccine\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-19T20:07:52+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-11-assignment/",
    "title": "Assignment",
    "description": "Exploring and Protoyping Visualizations for Understanding News Corpus.",
    "author": [
      {
        "name": "Atticus Foo",
        "url": "https://public.tableau.com/profile/atticusfoo#!/"
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOvervieww: Difficult to sense-make news headlines in digital landscape\r\nGoal: Provide readers with useful snapshots of news headlines\r\nData: Local News Reports in 2020\r\nLiterature review:\r\nImportance of analyzing unstructured news data\r\nImpact of News Media on our lives\r\nImpact of Social Media and Echo Chambers\r\nLimited resources for visualising unstructured data\r\n\r\nApproach\r\n1. Data extraction and Wrangling\r\nData Extraction\r\nData Exploration Before Cleaning\r\nScoping the dataset\r\nDataText Pre-Processing and cleaning\r\n\r\n2.Topic Modeling Methods\r\nWhy Latent Dirichlet Allocation (LDA)\r\nCriticisms of LDA\r\nDifferent methods and packages for LDA\r\nIssues faced\r\nPackage Selected: text2Vec\r\nVisualizing Results\r\n\r\n3.Presenting Snapshot\r\n4.Objective 1: Sorting through and identifying key events in large corpus of news\r\n5.Objective 2: Providing greater detail and context about a topic / news source using sentiment\r\n6.Objective 2: Providing greater detail and context about a topic / news source using word networks and co-occurrences\r\nCo-Occurrences\r\n\r\n7.Objective 3: Providing alerts to unusual patterns in news analysis\r\n\r\nSummary of observations from exploration of R Packages\r\nProposed Sketch of Project Module 2: Contextual Insight to Topics\r\nClosing Reflections\r\n\r\nOnline NewsOvervieww: Difficult to sense-make news headlines in digital landscape\r\nMaking sense of the news in today’s landscape is extremely difficult because of:\r\nThe volume of information available to you via digital channels;\r\nSocial media bubbles may create echo chambers that only serve you only one aspect of news making it difficult to understand the full context;\r\nNews can be manipulated based on phishing (creating duplicates) and inflated engagement metrics campaigns.\r\nThis can be useful as news headlines can have very real impact on not just personal sense-making but also cause tangible effects. This can be seen from how media reporting can contribute to the public’s understanding of safe distancing measures during COVID-19 to how it can influence share prices on the stock market. The following blogpost is to establish the project’s objectives, explore current practices, and prototype methods in order to help readers navigate the digital news landscape.\r\nGoal: Provide readers with useful snapshots of news headlines\r\nAs such, the team’s goal is to build an accessible dashboard that can help readers of local news sites (both mainstream and non-mainstream) quickly navigate and understand a large corpus of news. This will be done by applying statistical methods to sort large volumes of unstructured data. The intended result is different snapshots that would allow readers to:\r\nSort through and identify key events in the large corpus of news events by:\r\nTime period;\r\nTopic and;\r\nKeyword\r\n\r\nUnderstand in greater detail the context around each theme and news source by\r\nSentiment and;\r\nWord structure\r\n\r\nBe alerted to unusual patterns\r\nIdentify anomalous events and articles based on engagement.\r\n\r\nAs such, while the presentation of the ‘snapshot’ may appear simple, the methods that would derive these insights are not. The approach taken for this project is based on a mixed of deep research of current statistical methods, domain knowledge of the local news reporting landscape and rigorous data wrangling.\r\nData: Local News Reports in 2020\r\nThe data selected for analysis is from 1 January 2020 to 31 December 2020. These local news sites are a mix of mainstream (Straits Times, Channel News Asia, AsiaOne) and non-mainstream (Mothership, MustShareNews, The Independent) sites. A few others such as (TODAY Online, The Online Citizen) were initially selected but removed from our observations. This will be explained at the data wrangling phase.\r\nPrrtData extracted from Prrt is extremely useful for the project as it provides an easy tool for the extraction of the article headline, corresponding URL and Facebook Engagement metrics (Likes, Shares and Comments). In the absence of actual webviewership data, Facebook engagement data is often used as a proxy as the dataset is more easily accessible. Facebook is also arguably the most widely used social media platform in Singapore> it is also still used by major news sites for information dissemination as well as resource for analysis and understanding of the virality of a news article and topic.\r\nLiterature review:\r\nImportance of analyzing unstructured news data\r\nWhile preparing for the project, we researched four different areas to understand the key areas that needed to be addressed. The main challenge of the project is that of dealing with large volumes of unstructured data (224,351 news headlines and 2,692,224 observations). There are also limited libraries we can call upon in R as text analytics packages for R on CRAN are limited in number. Visualization packages that avoid simple word cloud comparisons are far fewer in number. This is compounded by the fact that the project is very niche as it is set in the local context, and as such, would have little case study options readily available.\r\nDespite these constraints, the team is of the opinion that the project is a worthy one as there is an increasing need to help navigate the digital news landscape. As mentioned in the earlier section, news can have a very large and tangible impact on our lives and the increasing availability of data visualization tools within the realms of journalism can help readers make sense of the stories that are out there. However, as identified by Data Journalism, there is not enough being done to take advantage of this.\r\nFortunately, as news media tended to follow standardized schools of reporting (e.g. AP style Book), there was research available (predominantly) on different methods used to analyse western media. These methods could then be adopted for analysing Singaporean media. For instance, text cleaning and sentiment analysis could easily adopt the English packages as they were unlikely to contain colloquial lingua franca that was most used on local social networking sites. Furthermore, the use of standardised stylebooks for news reporting would help the Latent Dirichlet Model which the team intends to use, as the model relies on the probabilities of similar words across similar topic distributions.\r\nImpact of News Media on our lives\r\nAs identified by Bhargava, Bishop and Zuckerman, there is clear evidence of the influence of the news media and its news reporting. They posit that building tools can help readers quickly analyse content, influence and the spread of news. In turn, they will be better equipped to respond to the growing impact of visual (and viral) news. This group’s earlier study also demonstrated that mobile app usage frequency can be correlated to news events and reporting (Chee, Nam, Foo 2020).\r\nKouivunen Niemi and Masoodian echo this in their recent article, visualising narrative patterns in online news media. They argue that the new media and its reports can have “widespread repercussions in the public perception of past and present phenomena” and proposed “temporal visualizations for examining differences in media narrative patterns over time and across publications”. This is provide a tool or method to simply to report findings in an “easy to understand but effective way”. They also go further to lament how these type of (text) studies are “almost invariably presented in tables, or using simple graphs such as bar charts or line charts”.\r\nCo-occurrences across different textsImpact of Social Media and Echo Chambers\r\nIn recent years, increasing amounts of research (the echo chamber effect on social media have also point to the dangers of social media companies inadvertently creating echo-chambersin order to maximise engagement metrics. They do this by presenting readers content that they are likely already biased towards, thus increasing the propensity (Platforms like Facebook are designed to profit for human’s confirmation bias that they continue engagement with the social media platform.\r\nTo combat this, researchers propose that readers pay attention the news source to ensure authenticity and the corresponding engagement metrics in order to determine if an article has been manipulated to reach a wider audience (e.g. when the engagement ratios for a content does not appear to be natural. This happens typically when ‘likes’ or ‘shares’ are mobilised. As such, we intend to provide a range of 6 sources that readers can easily access when they are looking for information related to a topic. They are then able to explore the context surrounding a topic (i.e. sentiment and co-occurring words) and use engagement metrics to identify if there are anomalous articles.\r\nLimited resources for visualising unstructured data\r\nA commonly used method for visualising text is the use of word clouds . They often aim to represent the most commonly used words in a corpus by size but have limited utility at best (crf. when word clouds are not enough, and mis-representative of the data at [worst]https://towardsdatascience.com/word-clouds-are-lame-263d9cbc49b7). Often, they include common verbs that do very little to explain word in the context of the corpus of data (crf. Why word clouds harm insights.\r\nPhoto Credit: Shelby TempleOther interesting methods explored for text visualization build on Venn and Euler diagrams.\r\nEuler DiagramThey include work by Rodgers, Stapleton & Chapman (Visualizing sets with linear diagrams which demonstrate the intersection where words are shared by different sources or topics.\r\nVisualizing sets with linear diagramsA similar approach was adopted by Luz and Masood (a comparison of linear and mosaic diagrams for set visualization.\r\nComparison of different scalings of mosaic and lienar diagramsHowever, these methods were not entirely applicable for our project as these visualizations benefitted from a small number of documents that could be extensive in length. Our dataset however, consists of over 200,000 documents that were all very short in length (a news headline typically has up to 30 words).\r\nOther than visualising topics and their word occurrences in other texts, we also intended to learn methods to visualise the temporal evolution of news topics as we intended to allow readers to explore how news evolved across a full year in 2020. This, we felt, would be interesting as the COVID-19 pandemic allowed for us to track the evolution of a news topic across nearly 12 months. This was somewhat of an irregularity in news media as the typical news cycle operates on a much shorter cycle of 48 hours (or less). While researching on this, we came across interesting methods used by Sheidin & Lanir (Visualising Spatial-Temporal Evaluation of News Stories.\r\nNews of Iran firing ballistic missles around the worldThe visualisation presents an idea for a system that is able to analysis the spread and volume of news episodes across space and time. However, this is not entirely applicable to the dataset used in our project because the number of news sources in the study are low (6) and all based in Singapore. Another mitigating, but more minor factor, is the lack of time data in the dataset.\r\nApproach\r\nBased on the research, our proposed approach for the project would be to create three separate modules that would allow users to 1) explore the corpus to identify key events 2) understand the key events / topics in greater detail and 3) to be alerted to unusual patterns. While the eventual presentation will appear to be a simple (and ideally effective) report card of the media situation, it will be grounded at the backend by statistical methods and rigorous data wrangling.\r\n1. Data extraction and Wrangling\r\nData Extraction\r\nAs mentioned above, the dataset that we have extracted is taken from Prrt. This is because Prrt is unique to local news media publishers in Singapore (and Malaysia) and allows us to extract news headlines together with its corresponding social media engagement. For other types of news extraction, there are R packages that offer news headline extraction. These include Guardian API Media News and Newsmap.\r\nAn interesting package for news extraction and analysis is NewsFlow as it allows for tracking of the flows between offline and online news. It is an interesting case study of how quickly news spreads from the offline sphere to the online one, or vice versa.\r\nNewsFlowHowever, as the scope of the project is primarily concerned with online news, we will not be able to utilize the NewsFlow package for our analysis.\r\nData Exploration Before Cleaning\r\nBefore we started on data cleaning, we want to explore the dataset to identify if there are any potential issues (such as low or missing values) in the dataset. R provides many packages that can help do this (e.g. Dplyr, GGbetweenstats, Data Explorer and Janitor.\r\nWe will use Data Explorer to perform preliminary data exploration, to identify which areas to go into further depth. Data explorer allows for very quick EDA and feature reporting. For this step, we will need to install Tidyverse and DataExplorer.\r\ninstall.packages(“DataExplorer”, “tidyverse”, dependencies = TRUE)\r\nlibrary(DataExplorer, tidyverse)\r\n#read csv file using tidyverse from your local source folder\r\nmedia_explore <- read_csv(“Data/all_media.csv”) \r\n#check data for missing values\r\ndata(media_explore) \r\nplot_missing(media_explore) \r\n#create a pdf report \r\ncreate_report(media_explore) \r\nDataexplorerResults from the Data Explorer also show little issues with missing values as the dataset is relatively complete and that the data fields are in the correct data types (e.g. chr and num)\r\nDataexplorerDataexplorerScoping the dataset\r\nWhile we wanted a good mix of news sites in our dataset for analysis, we had to be judicious in pruning four sites from our dataset. These were Rice Media, The Online Citizen, TODAY Online and Yahoo SG. These sites were removed for the following reasons: - Rice Media did not publish every day and contributed to a very small amount of articles published in 2020; - The Online Citizen had long a periods of missing data from the months of April 2020 to August 2020; - TODAY Online tended to be similar in style and scope as Channel News Asia (being that they both operated as Mediacorp’s English news websites); - Yahoo SG was removed as they performed primarily as a content aggregator with the bulk of their content republished from news wires such as Reuters.\r\nTo further refine the dataset that we wanted to analyze, we decided to set a threshold on engagement data, and include in the dataset, articles with at least 10 interactions. Doing this reduces the number of articles in the dataset by 131,235 (or close to 58%) and removes articles that were: - Removed quickly after A/B testing; - Removed quickly after editorial corrections; - Republished from international news wire agencies and therefore had low to no engagement.\r\nAs we are focused with news articles that have some reach, we removed articles with lower than 10 Facebook engagement from our analysis. A remaining 93,116 articles was available for analysis.\r\ntest <-Cleaned %>%\r\n  filter(Total>10)%>%\r\n  select(c(1,10))\r\ntest\r\nDataText Pre-Processing and cleaning\r\nBefore we can begin text processing, we will need to employ text cleaning to reduce noise to optimize results. Examples of ‘noise’ include special characters, punctuation, common words and typographical errors. During this process, words will also be converted to lower case characters so that same words can all be identified by the computer as the same entity. Useful packages for doing this is the StringR function within the TidyVerse package, Spacyr which utilizes Python function using Reticulate, Quanteda, Qdap or TM. The method we are using below uses the Stringr package. Stringr is a useful package for working with strings and regular expressions using its pattern matching function and can be used for character manipulation to do text pre-processing.\r\n\r\nlibrary(stringr)\r\n#using stringr and textclean for cleaning\r\nmedia_processed$Cleaned<-tolower(media_explore$Text)%>%#convert to lowercase\r\n  replace_contraction() %>% #lengthening words (eg,isn't -> is not)\r\n  replace_word_elongation() %>% #reducing informal writing (eg,heyyyyyyy -> hey)\r\n  str_replace_all(\"[0-9]\", \" \") %>% #removing numbers\r\n  str_replace_all(\"[[:punct:]]\",\"\")%>%#remove punctuation\r\n  str_replace_all(\"covid|wuhan virus\",\"coronavirus\")%>%#word substitution\r\n  str_squish()%>% #reduce repeated whitespace \r\nstr_trim#removes whitespace from start and end of string\r\n#check data \r\nview(media_processed)\r\n#we will first need to create a Dataframe source from a dataframe\r\nmedia <- DataframeSource(media_explore) \r\n###################### not in use ############################\r\n#we can now create the corpus using tm \r\ncorpus <- Corpus(media)\r\n#remove words that at not English \r\nskipWords <- function (x) removeWords (x, stopwords(“english”)) \r\n#changing all words to lower case\r\ncorpus <- tm_map(corpus, FUN = tm_reduce, tmFuns = list (tolower))\r\n#remove \r\ncorpus < - corpus, FUN = tm_reduce, tmFuns = list(skipWords, removePunctuation, stripWhitespace, removeNumbers, stemdocument))\r\n\r\nA popular method in data cleaning for text is the use of Term Frequency – Inverse Document Frequency or TD-IDF which is a pre-processing method that helps surface ‘interesting’ terms in a corpus and reduces importance of commonly used words that appear frequently in the corpus. However, as identified earlier, the nature of standardized style of news reports around common events will inevitably engender the occurrence of common terms. These ‘common terms are therefore important and should not be reduced in ‘weightage’ when analyzing a corpus of text. Because of this, we will not be adopting the TF-IDF method for text cleaning for the purposes of this project and will proceed on to the next step, Topic Modeling.\r\n2.Topic Modeling Methods\r\nWhy Latent Dirichlet Allocation (LDA)\r\nTo help readers make sense of key events or topics in the large corpus of data, the project will take advantage of the Latent Dirichlet Allocation (LDA) model. This is because LDA is a generative statistical model “that allows for sets of observations to be explained by unobserved groups that explain why some part of the data are similar”. The LDA method has also been successfully adopted for other fields such as banking.\r\nCriticisms of LDA\r\nAlthough LDA has been criticized for issues with overfitting and lacks an intrinsic method for choosing a number of topics it is a method that is extremely advantageous for the purposes of our study. This is because the method itself is unsupervised and allows for the dashboard to assist readers in identifying topics that are ‘unknown-unknowns’ even though some of the topic models derived may not be immediately clear. As we are unable to ‘predict’ news events, we are unable to pre-define topics for analysis and presentation in the snapshot. As the LDA method is unsupervised and can handle ‘unknown-unknowns’, the proposed dashboard is thus scalable and reproducible as it will be able to handle new datasets containing new or future news articles.\r\nDifferent methods and packages for LDA\r\nAfter selecting the LDA method, we identified different methods and packages that can be used. These include the traditional LDA, word2vec and the newer text2vec. Both word2vec and text2vec methods build on top of the LDA model and are less resource demanding and has a smaller memory footprint because it does not need to perform a lookup over an associative array. It does this with feature hashing that maps features into a more compact space (crf. original research paper by Yahoo).\r\nIssues faced\r\nCreating topic models using TidyText and its associated packages (Quanteda and TM) was not ideal for 220k documents as the document term matrix would be prohibitively large. Each iteration of LDA modeling using this method would take longer (e.g. 5,000 iterations took about 5hrs on our laptops).\r\nWe were also not able to properly harness memory allocation packages such as (Big Memory and Parallel. The processes were still lengthy even after we removed infrequent words from the Document Term Matrix. More importantly, the results were not easily reproducible as the LDA model is based on random seeds, it would return a different result (if we did not set a seed) after every 5 hours of running it.\r\nTidy Text Topic ModelingPackage Selected: text2Vec\r\nDue to the reasons outlined above, we proceeded with the Text2Vec package for Topic Modelling. The steps on topic modelling using text2vec can be found below:\r\n#install package \r\ndevtools::install_github('dselivanov/text2vec\r\nlibrary(text2vec) \r\n#tokenize words\r\ntokens = media_processed$cleaned) \r\ntokens = word_tokenizer(tokens)\r\nit = itoken(tokens, ids = media_processed$docid, progressbar = FALSE)\r\nv = create_vocabulary(it)\r\n#remove very common and uncommon words \r\nv = prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)\r\nvectorizer = vocab_vectorizer(v)\r\n#constructing a Document Term Matrix \r\ndtm2 = create_dtm(it, vectorizer, type = \"dgTMatrix\")\r\n# Creating LDA for 20 topics\r\nlda_model = LDA$new(n_topics = 20, doc_topic_prior = 0.1, topic_word_prior = 0.01)\r\ndoc_topic_distr = \r\n  lda_model$fit_transform(x = dtm2, n_iter = 1000, \r\n                          convergence_tol = 0.001, n_check_convergence = 25, \r\n                          progressbar = FALSE)\r\n# identifying the proportion of word distribution\r\nbarplot(doc_topic_distr[1, ], xlab = \"topic\", \r\n        ylab = \"proportion\", ylim = c(0, 1), \r\n        names.arg = 1:ncol(doc_topic_distr))\r\nDoing this will give the distribution for the topic distribution for the first document#getting top 10 words for each topic based on lambda 0.4\r\nlda_model$get_top_words(n = 10, topic_number = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 19L, 20L), lambda = 0.4)\r\nWe will now get the LDA topic model results, based on 10 keywords for 20 topics using the lambda 0.4.\r\n20 Topics#apply learned model to new data \r\nit = itoken(test$Cleaned[4001:5000], tolower, word_tokenizer, ids = test$docid[4001:5000])\r\nnew_dtm =  create_dtm(it, vectorizer, type = \"dgTMatrix\")\r\nnew_doc_topic_distr = lda_model$transform(new_dtm)\r\n#Check for perplexity. The lower the score, the better\r\nperplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)\r\nVisualizing Results\r\nAfter retrieving the topics from text2vec, we would now need to visualize the results. One interesting visualization for topic modeling was the termite model created by a team at Stanford University. As seen in the screenshot below, the visualization is clean yet simple, and clearly communicates the size and overlap of keywords between topics.\r\nTermiteThis visualization below is the alternative using LDAviz.\r\n Visualization below may not be visible in Chrome as chrome does not allow running of local files.\r\nWe want to save the topics derived for further analysis using the code below. This will allow engagement data derived from the articles to be visualized in an aggregated fashion as topics, instead of per article (which is not ideal as there are 96k articles).\r\ngroup_by(day)%>%\r\nadd_column(topic = topic ,.before = 'Date’))\r\n3.Presenting Snapshot\r\nTo achieve the objective of presenting a useful snapshot or report card of the media situation, we explored different packages such as [SunburstR(https://cran.r-project.org/web/packages/sunburstR/index.html), GGVIS , Esquisse and D3PartitionR which can read large volumes of data quickly (using FREAD).\r\nD3PartitionR ExampleHowever, after running some tests, we found the visualizations for SunrburstR and D3Partition to be unsuitable. This is because of the size of the documents found in the corpus being too large to visualize using these methods. The speed of interactions was slow, and the visualization outcomes poor due to the variety of colors used for the range of variables found in the dataset.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAs such, despite the packages ability to quickly read data (eg. using FREAD) and their ability to present topline data in an interactive and easy to understand manner, it was not suitable for the purposes of our project.\r\n4.Objective 1: Sorting through and identifying key events in large corpus of news\r\nThe alternative found was using a new package called CorporaExplorer. Using the package creates a Shiny App that allows us to quickly input a corpus of documents that can be easily searched by date or keyword. The output is an interactive heatmap “where each tile represents one document” as well as each document’s corresponding metadata. Using the data we have loaded, we can easily prepare the corpus for visualization using this package.\r\ninstall.packages(“corporaexplorer”)\r\nlibrary(corporaexplorer)\r\n#preparing the data \r\ncorpus_explorer <- \r\n  media_explore,\r\n  date_based_corpus = TRUE,\r\n  grouping_variable = \"Topic\", #this is skipped if date is TRUE\r\n  within_group_identifier = \"Source\", #this is also skipped if date is TRUE \r\n  columns_doc_info = c(\"Topic\", \"Headline\", \"URL\", \"Source\", \"Total\", \"Likes\", \"Shares\", \"Comments\"), #appending the relevant metadata from the dataset\r\n  corpus_name = \"Local Media Coverage in 2020\", #naming the dashboard\r\n  use_matrix = TRUE, \r\n  matrix_without_punctuation = TRUE,\r\n  tile_length_range = c(1, 25),\r\n  columns_for_ui_checkboxes = TRUE,\r\nWith the data prepared, we can now run the Shiny App to explore the corpus\r\nexplore( \r\n  corpus_explorer,\r\n  search_options = list(optional_info = TRUE), #option for keywords in search\r\n  ui_options = list(font_size = \"10px\"), #size of font in app\r\n  plot_options = list(max_docs_in_wall_view = 100000) #maximum number of documents in the list \r\n)\r\nThe screenshot above demonstrates the corporaexplorer using the dataset of local media articles published in 2020The Coporaexplorer package is able to handle the 91k articles quickly and allows for readers to search the entire corpus by (up to 5) keywords. Selecting a specific day in the Coporaexplorer plot allows readers to then discover more about the selected article / document. This includes the corresponding engagement data regarding the article’s reach and URL and allows readers to meet the goal of quickly sorting through a large corpus of news events by time, topic and keyword.\r\nMore detail regarding the module for Objective 1 can be found at the project blog here.\r\n5.Objective 2: Providing greater detail and context about a topic / news source using sentiment\r\nThe second objective to use statistical methods to provide greater detail and context around each topic and news source can be done through sentiment analysis. This will allow readers to understand the value of the sentiment attached to the topic or media source and observe its changes across time.\r\nThere are several sentiment analysis packages available including syuzhet and sentimentr. Lexicons that are available for sentiment analysis include “Bing”, “AFINN” and [“NRC”])(https://hoyeolkim.wordpress.com/2018/02/25/the-limits-of-the-bing-afinn-and-nrc-lexicons-with-the-tidytext-package-in-r/).\r\ninstall.packages(“sentimentr”, “GGally”, “glue”, “reshape2”)\r\nlibrary(“sentimentr”, “GGally”, “glue”, “reshape2”)\r\n#unnest tokens for analysis \r\nmedia_words <- unnest_tokens(media_explorer, word, text) \r\nview(media_words) \r\n#use bing lexicon to get media sentiment \r\nmedia_sentiment_new <- media_words %>%\r\n  inner_join(get_sentiments(\"bing\")) %>%\r\n  count(source, date, sentiment) %>%\r\n  spread(sentiment, n, fill = 0) %>%\r\n  mutate(sentiment = positive - negative)\r\n#use ggplot to plot sentiment over time \r\nggplot(media_sentiment_new, aes(date, sentiment, fill = source)) +\r\n  geom_col(show.legend = FALSE)\r\nOverall sentiment analysisArticle sentiment was most in late January, presumably about the same time the first cases of COVID-19 were found in Singapore – but later trended upwards towards the end of the year. From this analysis, readers can infer that the overall sentiment and pandemic situation improved over the span of the year.\r\nYou can change the sentiment plot to show specific media by using ggplot’s facet wrap feature using the code below:\r\n#use ggplot to plot sentiment over time \r\nggplot(media_sentiment_new, aes(date, sentiment, fill = source)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~source, ncol = 2, scales = \"free_x\") #use facet to show media sentiment across different sources \r\nOverall sentiment analysis by mediaBased on the chart, we can see that the Straits Times and Channel News Asia publishes more articles with negative keywords from the ‘Bing’ lexicon. This could also give insight to the type of news in their respective news coverage (e.g. news about accidents, crime, death tend to feature more negative keywords) as full fledged news sites could cover a wider range of news stories compared to lifestyle news focused new sites such as Must Share News. To better understand this hypothesis, we can use the word cloud functions and word structure (network and n-gram) analysis found in the later section of this blog.\r\nWe are also able to use word cloud to visualize positive and negative words used most frequently in media coverage.\r\nSentiment Word CloudAs mentioned earlier, the word cloud visualization here is not particularly useful as it provides very little value as to what the words’ context are. In the visualization above, it does a bit more to separate negative and positive words but this can be a bit arbitrary. For instance, “Trump” here is seen as positive when it refers to the verb “trump” rather than the former President Donald Trump.\r\nFinally, we are able to also chart news headline sentiment to the SGX to identify if there are any correlations between news sentiment and SGX stock prices. To do this, we will first need SGX data which can be downloaded from Yahoo Finance.\r\ninstall.packages(\"gridExtra\", \"plotly\", \"dygraphs\", \"xts\", \"zoo\")\r\nlibrary (\"gridExtra\", \"plotly\", \"dygraphs\", \"xts\", \"zoo\")\r\n\r\n#read downloaded SGX data\r\nTeststock3 <- read_csv(\"Data/sgx.csv\")\r\n#format date\r\nDate=teststock3$Date <- as.Date(as.character(teststock3$Date,\"%Y-%m-%d\"))\r\n#check date format \r\nclass(Date)\r\n#join sentiment and SGX datasets by date \r\nteststock3 <- teststock3 %>%\r\n  left_join(media_sentiment_new, by = c(\"Date\" = \"date\"))     \r\nview(teststock3)\r\n#convert data to xts format for dygraph\r\nDate=teststock3$Date <- as.xts(ts(start = c(2020-1-1), endc(2020teststock3$Date)\r\nOpen=teststock3$Open <- as.numeric(na.locf(teststock3$Open))\r\nClose=teststock3$Close <- as.numeric(na.locf(teststock3$Close))\r\nHigh=teststock3$High <- as.numeric(na.locf(teststock3$High))\r\nLow=teststock3$Low <- as.numeric(na.locf(teststock3$Low))\r\nSentiment=teststock3$sentiment <- as.numeric(na.locf(teststock3$sentiment))\r\n#bind coloums and convert to xts format required by Dygraph\r\nz=cbind(Open,Close,High, Low, Sentiment)\r\nnewdata=xts(z,Date)\r\n#plot graph using dygraph\r\ndygraph(newdata, main = \"SGX VS Sentiment of Local News Headlines\") %>%\r\n  dyEvent(\"2020-01-23\", \"First COVID-19 Case in SG\", labelLoc=\"bottom\")%>% #plotting events\r\n  dyEvent(\"2020-01-29\", \"First Travel Restrictions to SG\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-03-27\", \"Circuit Breaker Phase 1 starts\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-05-19\", \"Circuit Breaker Phase 1 Ends\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-07-10\", \"Polling Day\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-08-09\", \"National Day\", labelLoc=\"bottom\")%>%\r\n  dyEvent(\"2020-11-09\", \"Biden confirmed as Winner\", labelLoc=\"bottom\")%>%\r\n  dyShading(from = \"2020-03-27\", to = \"2020-5-19\", color = \"#FFE6E6\") %>%\r\n  dyHighlight(highlightCircleSize = 5, \r\n              highlightSeriesBackgroundAlpha = 0.2,\r\n              hideOnMouseOut = FALSE) %>%\r\ndyRangeSelector()\r\n\r\n\r\n\r\nThe resulting plot from Dygraph provides useful interactive features that allows readers to pan and select the time period for analysis. They are also able to mouseover key data points to derive the corresponding SGX prices. To understand the tangible impact news events and sentiment has on the stock market, we conducted a multiple linear regression to observe the results. The codechunk for this can be found below:\r\n#create columns for testing MLR\r\nallCols <- colnames(teststock3)\r\nregCols <- allCols[!allCols %in% c(\"Open\",\"High\",\"Low\", \"Adj Close\")]\r\nregCols <- allCols[!allCols %in% c(\"Open\", \"High\", \"Low\")]\r\n#create regression formula \r\nregCols <- paste(regCols, collapse = \"+\")\r\nregCols <- paste(\"Close~\",regCols, collapse = \"+\")\r\nregCols <- as.formula(regCols)\r\n#call MLR \r\nTeststock3.lm <- lm(regCols, data = teststock3) \r\nSummary(teststock3.lm)\r\nMLR ResultsThe results for the multiple regression model demonstrate that there is significant collinearity between negative media sentiment and the closing stock prices of the SGX daily. As seen in the results, news events and headlines should be analyzed in greater detail because they can have very real and tangible impact.\r\n6.Objective 2: Providing greater detail and context about a topic / news source using word networks and co-occurrences\r\nWord network\r\nAnother method explored for allowing users to read in greater detail, the different topics and news sources, is the analysis and visualization of word structure. This can be done by analyzing the word graph or word networks relating to the topic or media source. Word analysis using statistical methods can be very use in helping to track and visualize the evolution of news, and demonstrate how differently each media has covered a topic.\r\nUsing R packages such as tidytext, dplyr, igraph and ggraph, we can plot the word networks related to different topics in the corpus.\r\ninstall.packages(“tidytext”, “dplyr”, “igraph”, “ggraph”)\r\nlibrary(“tidytext”, “dplyr”, “igraph”, “ggraph”)\r\n#creating bigrams\r\nmedia_bigrams <- media_explore %>%\r\n  unnest_tokens(bigram, text, token = \"ngrams\", n=2)\r\nmedia_bigrams\r\n#sorting bigrams \r\nmedia_bigrams %>%\r\ncount(bigram, sort = TRUE)\r\n#splitting bigrams and removing stop words \r\nbigrams_separated <- media_bigrams %>%\r\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\r\nbigrams_filtered <- bigrams_separated %>%\r\n  filter(!word1 %in% stop_words$word) %>%\r\nfilter(!word2 %in% stop_words$word)\r\n\r\n#new bigram counts \r\n# new bigram counts:\r\nbigram_counts <- bigrams_filtered %>% \r\n  count(word1, word2, sort = TRUE)\r\nbigram_counts\r\n#graphing word network  \r\nbigram_counts\r\nbigram_graph <- bigram_counts %>%\r\n  filter(n > 5) %>%\r\n  graph_from_data_frame()\r\nbigram_graph\r\nset.seed(2017)\r\nggraph(bigram_graph, layout = \"fr\") +\r\n  geom_edge_link() +\r\n  geom_node_point() +\r\ngeom_node_text(aes(label = name), vjust = 1, hjust = 1)\r\nWord NetworkUsing the word network plotted above allows readers to identify the words that were frequently used together and gives readers more context to the topic (ie. GE2020). However, this method is not the most intuitive and its presentation left a lot to be desired.\r\nReferenced earlier in the blog, is Masood’s work that we felt was very useful in visualizing co-occurring words across different media sources. Visualizing this could be very useful as readers can quickly infer if certain keywords or entities have been left out when analyzing topics.\r\nCo-occurrences across different textsUnfortunately, this visualization of presenting sets is more useful when the number of documents in the text is small. Visualizing this over 96k articles will be less helpful, even after aggregation into different topics. An alternative approach of visualizing co-occurrence is explore in the next section.\r\nCo-Occurrences\r\nFinally, using the tri-grams identified in the text, we are able to visualize how any two words in a selected corpus is used, where they are similar and where they diverge. Research into empirical linguistics have identified how the frequency of words occurring together can give insight and meaning to the treatment of a topic and the ‘closeness’ of association between entities. This can help readers understand the context to how words in media coverage was used when compared to each other.\r\nFor this segment, we were inspired by the work of Chris Harissson and Emil Hvitfeldt. We will need to call on the packages of purrrlyr for this next visualization. The code below is attributed to Emil Hvitfeldt.\r\ninstall.pacakges(“purrrlyr”)\r\nlibrary(purrrlyr)\r\nn_word <- 20\r\nn_top <- 150\r\nn_gramming <- 3\r\n#creating trigrams\r\ntrigrams <- tibble(text = media_explore$Text) %>%\r\n  unnest_tokens(trigram, text, token = \"ngrams\", n = n_gramming)\r\n#select the start words you want to compare\r\nstart_words <- c(\"covid\", \"coronavirus\")\r\nAs seen in the visualization above, the word network can be elegantly displayed and compared with each other to see how they unfold more often in media coverage. From the visualization, we can infer that the word PAP is more often related to a wider range of words in media coverage compared to WP. We hope to integrate this function as a module in the Shiny Dashboard for the project.\r\nThe proposed sketch and detail regarding the module for Objective 2 can be found in the later section of this blog here.\r\n7.Objective 3: Providing alerts to unusual patterns in news analysis\r\nTo meet objective 3 for the project, we researched how technology companies such CrowdTangle and Newswhip used engagement figures in order to predict the likelihood articles go viral. Newswhip utilizes a method of measuring velocity by tracking the speed of engagements and measures if it is overperforming when compared to the average interaction speed of other articles from the same publisher.\r\nThis can be useful for us to visualize anomalies in time series using packages in R. Sorting this by media type or by topic, can help to identify if there are key articles or events that need to be explored first, amongst the influx of articles that being disseminated at any point of time.\r\nMore detail regarding the module for Objective 3 can be found at the project blog here (url)\r\nSummary of observations from exploration of R Packages\r\nTo summarize, we will be adopting packages for three different modules to meet our objectives of helping readers to better understand news events in a large corpus of headlines, create snapshots that are based on rigorous statistical methods to understand quickly the context surrounding a topic and finally to see red-flags / or anomalous instances in the news corpus.\r\nProposed Sketch of Project Module 2: Contextual Insight to Topics\r\nBased on the research work, methods and proto-typing found above the proposed dashboard for the Project’s Module that will help readers contextualize news topics and sources can be found in the sketch below.\r\nSketch for proposed moduleThe intended module seeks to provide media sentiment using the ‘bing’ lexicon and give insight into topics by allowing them to observe the tri-grams most frequently used with the topic keyword. Users will also be able to select the topic or media source in order to create a ‘snapshot’ that would present them with the relevant information.\r\nClick on the respective links to find out more about modules 1 and 3. The integrated Shiny dashboard for the modules will be presented at a later date.\r\nClosing Reflections\r\nWhile there are many aspects to explore to help make sense of the news corpus, we have selected packages and methods that would best suit a large volume of unstructured data and contain interactive user functions to allow them to quickly navigate the corpus and create snapshots around topics they are interested in.\r\nFuture work could include advance methods for dynamic topic modeling, which would be helpful in identifying how a topic evolve across time (e.g. from Wuhan Virus, to Coronavirus to Covid-19) as well as building on word-graphing and word network techniques for the identification of duplicate articles or ‘fake news’. Improvements can also be made to the user experience by customizing CRAN packages that are used for the purposes of this project.\r\nThank you for reading up to this point.\r\nThis blog is a data visualization assignment for the MITB programme at the Singapore Management University.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-11T16:40:00+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-20-dataviz-makeover-1/",
    "title": "Dataviz Makeover 1",
    "description": "Re-imagining MOM's Labour Force in Singapore 2019 Report in Tableau",
    "author": [
      {
        "name": "Atticus Foo",
        "url": "https://public.tableau.com/profile/atticusfoo#!/"
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\n#1 Asessment of visualisation based on clarity and aesthetics\r\n1.1 Clarity\r\n1.1.1 Annotations do not match the data presented in the table\r\n1.1.2 Which dataset is the visualisation referring to?\r\n1.1.3 The main objective of the visualisation is not clear\r\n\r\n1.2 Aesthetics\r\n1.2.1 Labelling of Figure titles and captions\r\n1.2.2 Not ideal for readers with Colour-Vision Deficiencies (CVD)\r\n1.2.3 Directing focus with highlights\r\n\r\n\r\n#2 Sketch of Alternate Design\r\n#3 Proposed Visualisation in Tableau\r\n#4 Step-by-step guide\r\nData Preparation\r\nCharting Visualisation\r\n\r\n#5 Insights from the visualisation\r\n5.1 Aging Labour Force in Singapore\r\n5.2 Government Policy has an impact on LFPR\r\n5.3 Dramatic shifts in gender demographic between 2009 and 2019\r\n\r\n\r\nOverview\r\nThe Labour Force in Singapore 2019 is an annual report on Singapore’s labour market produced by the Manpower Research and Statistics Department. As part of the assignment, we will attempt to ‘makeover’ a chart depicting the resident labour force by age. The following sections of this makeover attempt can be found below:\r\n#1 Assessment of visualisation based on clarity and aesthetics\r\n#2 Sketch of alternate design\r\n#3 Proposed visualisation in Tableau\r\n#4 Step-by-step guide\r\n#5 Insights from the visualisation\r\n#1 Asessment of visualisation based on clarity and aesthetics\r\n“With population ageing and sustained rise in LFPR of older residents, the share of residents aged 55 & over in the labour force rose substantially from 16% in 2009 to 25% in 2019. Meanwhile, the share of resident labour force aged 25 to 54 declined from 75% to 67% even as their LFPR increased, as the population cohorts moving into these age bands were smaller than those who moved out due to lower birth rates. As a result the median age of residents in the labour force rose from 41 years in 2009 to 44 years in 2019.”\r\nWithout the context of the primary communication intent of the above chart, it is difficult to provide an assessment of how it can be improved. As such,the critique below will be based solely on the rubrics of clarity and aesthetics.\r\n1.1 Clarity\r\n1.1.1 Annotations do not match the data presented in the table\r\n• The selected age bands grouped in the annotations are not shown in the visual data - it is hence difficult to match the description with the visual. (e.g. 16% in 2009 = 8+4.6+1.8+1.2 = 15.6) and (25% in 2019 = 2019 10.2+7.3+4.5+2.7=24.7).\r\n• The current data or visualisation does not show the LFPR increase for the resident labour force aged 25 to 54 described in the annotation.\r\n1.1.2 Which dataset is the visualisation referring to?\r\n• The annotations describe Labour Force Participation rate - however the data in the visualisation demonstrates data relating to resident labour force. This makes it difficult to follow the description.\r\n• As the data only shows proportion of the labour force, it is not possible to determine the total volume of the labor force - to see if it has grown or shrunk from 2009 to 2019.\r\n1.1.3 The main objective of the visualisation is not clear\r\n• If the main aim is to demonstrate a) aging labor force, and b) increase in LBFR for 55 and over, the current chart does not show the movement across the decade.\r\n• May be able to do bracketing to show the movement within the age groups.\r\n1.2 Aesthetics\r\n1.2.1 Labelling of Figure titles and captions\r\n• The chart adopts a 538 style of visuals, with the the chart’s Title and Subtitle nested in the top left corner of the visualisation. While it is a trendy style to imitate, it is not executed well here as it the caption ‘per cent’ describes neither the title nor the graphic.\r\n• Instead, the subtitle ‘per cent’ could be moved closer to the table, so as to more clearly indicate that the data in the table are percentages.\r\n• The caption could then be changed to indicate that the graphic is intended to present the key point in the annotation - which is the shift in labour force demographic from 2009 to 2019.\r\n• The median of the graphic uses a continuous representation whereas the x axis is based on a categorical variable. This conflation is technically inaccurate and may make it confusing for the reader.\r\n1.2.2 Not ideal for readers with Colour-Vision Deficiencies (CVD)\r\n• In terms of aesthetics, the colour choice of grey and blue may be difficult to distinguish, especially for those with colour-vision deficiency (deuteranomaly).\r\n• It would be more helpful to refer to CVD safe colour scales so as to reach out to a wider group of readers. CVD simulators can be found [online](https://www.color-blindness.com/coblis-color-blindness-simulator/.\r\n• However, there is a lot of restraint in the terms of the use of colours, and this helps keep clutter to a minimum. This is generally good practice as it allows the reader to easily discern the main points.\r\n1.2.3 Directing focus with highlights\r\n• There are two vertical reference lines to show the change in the median age from 41 years old to 44 years old. A visual cue could have been added to show the shift. This can be done in the form of a small arrow.\r\n• It also does not show the largest changes in each cohort (ie. 15-19 remained the same).\r\n• At the same time, within each cohort, changes could be made to show whether the cohort witnessed an increase or decrease.\r\n#2 Sketch of Alternate Design\r\nSource: Sketch of alternate designThe sketch above highlights the three key proposals for change from the original visualisation:\r\n• The first is to improve the layout of titles and labels on the axis. The intent to provide clear text labels while keeping the visualisation clutter-free by reducing duplication or irrelevant information.\r\n• Secondly, as the main objective is to demonstrate the key shifts from 2009 2019, a decision was made to select LFPR only.\r\n• Lastly, we intend to include a time series – since the data is provide by MOM – in order to identify if the shifts in LFPR can be attributed to key events during that decade.\r\n#3 Proposed Visualisation in Tableau\r\n\r\n\r\n\r\nFigure 1: Data Re-presented.\r\n\r\n\r\n\r\nThe visualisation was built using Tableau Desktop Professional Edition 2020.40.\r\nThe key change made in the data visualisation is to include key events and its possible impact on LFPR. This means utilising a time series instead of just comparing two points of data, 2019 and 2009. Based timeline, it is likely policy changes have directly led to changes in LFPR.\r\nIn addition, a feature that was integrated into the LFPR dataset was the gender demographic as we wanted to examine if policy shifts had an impact on male and female workers in the population.\r\nLastly, visual changes were made to ensure consistency and clarity when comparing LBFR. A greyscale was also adopted for the demographic age groups, in order to suggest Singapore’s aging labour force.\r\n#4 Step-by-step guide\r\nThe following steps will provide a brief overview of how the visualisation was completed.\r\nData Preparation\r\nThe data used for the visualisations were taken from the Ministry of Manpower’s website - Resident Labour Force Participation Rate by Age and Sex 2009 - 2019 June.\r\nSteps\r\nData Preparation\r\nData in excel was imported via Tableau\r\n\r\nData was examined using Tableau’s data viewer. As only certain age groups was needed, these were selected and filtered out for export.\r\n\r\nData was then exported to Excel and manually adjusted. This is because we intended to use gender as an additional category. As such, data in the other sheets were merged together to form a combined worksheet.\r\n\r\nLastly, the adjusted Excel document was imported again in Tableau. The data fields was cleaned to ensure that the values were accurately represented\r\n\r\nCharting Visualisation\r\nNext, with the data prepared, Tableau’s Polaris was used to visualise the worksheet\r\nSteps\r\nCharting Visualisation\r\nAfter adding age group and year to columns and LFPR and it’s duplicate to rows, you should get a graph that looks like this\r\n\r\nRight click on the graph and select dual axis to combine both charts. Now syncrhonise and edit the axis\r\n\r\nAn important step is to ensure that the axis were fixed - as there are null values inserted to create a separate category (total & gender)\r\n\r\nAdd gender to the color filter to adjust the graph’s colour palette\r\n\r\nAdd annotations to highlight key events in the timeline\r\n\r\nWe added a reference line in order to add visual cues to the change in female worker (55 - 64)\r\n\r\nFinally, edit the caption below the X axis to credit source\r\n\r\n#5 Insights from the visualisation\r\n\r\n\r\n\r\nFigure 2: Data Re-presented.\r\n\r\n\r\n\r\n5.1 Aging Labour Force in Singapore\r\n• There is an observe shift in Singapore’s LFPR, with more elderly workers from the ages of 55 and above remaining active in the population’s labour force.\r\n• This shift is most observable in the age groups of 55 – 64 with an increase in 9.3 percentage points from 2009 and 65 and over with an increase in 11.5 percentage points in the same period.\r\n• This is likely to increase in the next decade, especially with proposed changes to retirement age (raise from 62 to 65 by 2030) and re-employment age (raise from 67 to 70 by 2030)\r\n5.2 Government Policy has an impact on LFPR\r\n• Government policy introduced in the last decade has allowed Singapore’s aging population provide more protections to older workers.\r\n• This is necessary to protect our aging workforce, by legislating that companies extend their retirement and re-employment age, to ensure that older workers in Singapore can continue to work and support themselves in old age.\r\n• As there was intense debate in 2013 regarding Singapore’s Population White Paper, it would be interesting to integrate foreign worker data to understand better how Government measures have protected Singaporean workers – whilst ensuring a viable labour force in Singapore.\r\n5.3 Dramatic shifts in gender demographic between 2009 and 2019\r\n• Based on the visualisation, the most dramatic changes from 2009 to 2019 is the increase in female workers in the LBFR across three different age groups from 25 years upwards.\r\n• This could be attributed to the trend of dual income families from the turn of the century. More recently, these changes could be attributed to changes to improve protections to female workers following the CEDAW report in 2011. Changes to protect working mothers was also introduced together with the Employment Act amendments in 2013.\r\n• Of note, is the largest increase in the older female demographic (as high as 15.6 percentage pts), likely due to protections made for older workers guaranteeing their employment past the ages of 62.\r\nThank you for reading up to this point!\r\nThis blog is a data visualisation assignment for the MITB programme at the Singapore Management University.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-01-20-dataviz-makeover-1/images/week1/Final5.png",
    "last_modified": "2021-01-28T19:14:02+08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-my-data-makeover-1/",
    "title": "Dataviz Makeover 1",
    "description": "Visualisation Makeover on MOM's Labour Force in Singapore 2019 Report in Tableau",
    "author": [
      {
        "name": "Chee Kah Wen Gerald",
        "url": "https://public.tableau.com/profile/gerald.chee1204#!/vizhome/DataVizMakeover1_16118032802250/Final?publish=yes"
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Introduction\r\n2. Critque on Clarity and Aesthetics\r\n2.1 Proposed Design\r\n\r\n3. Preparation \r\n4. Observations from Makeover Visualisation \r\n\r\n1. Introduction\r\nData collected by the Department of Statistics Singapore in the past 10 years (2009-2019) highlight a potential problem in our current society. It is revealed that Singapore’s population has begun to age, due to declining birth rates since 2010 . This correlates to having an aging working population where the labour force consists of a higher proportion of workers in the 50 and over age band as evident in the figure below.\r\nFIGURE 1: RESIDENT LABOUR FORCE BY AGEIn the context of Singapore, our labour force comprises of people above the age of 15 years old who are working or seeking work (regardless of employment status or citizenship). Using the dataset, Figure 1 was visualised by the Ministry of Manpower (MOM), Manpower Research and Statistics Department. The purpose of the document explores how a makeover on the exact same dataset would allow for an alternative graphical representation and the advantageousness of said work.\r\n2. Critque on Clarity and Aesthetics\r\nClarity\r\nAesthetics\r\nData presented does not match storytelling. (16% in 2009 and 25% in 2019 represents a summation of the percentages values for age bands of 55 and above)\r\nColour scheme is dull and is not impactful to the reader.\r\nNo y-axis presented; reader has to refer to table values to determine that the y-axis represents the percentage of labour force.\r\nHorizontal reference lines looks similar to a bar, this may mislead the reader to place emphasis on the age band 40-44.\r\nNo annotation for values in table. Reader is unable to tell if they represent absolute values or a percentage.\r\nNo ‘spikes’. A bar chart might be better to visually represent the ‘small’ changes in percentage values.\r\nAge bands mentioned are not visually shown.\r\n\r\nLabour Force Participation Rate (LFPR) increase not shown visually\r\n\r\nTitle depicts the graph as the proportion of labour force by age, thus it is unable to show change in LFPR as stated.\r\n\r\n2.1 Proposed Design\r\nFIGURE 2: PROPOSED VISUALISATION IN TABLEAU3. Preparation \r\nVisualisation for Figure 1 uses derived values from Table 7: Resident Labour Force Aged Fifteen Years And Over By Age And Sex, 2009 - 2019 (June). For the makeover, we are tasked to use Table 5:Resident Labour Force Participation Rate by Age and Sex 2009 - 2019 (June).\r\nSteps\r\nData Preparation\r\nData in excel was cleaned by removing unecessary data rows and formatting issues\r\n\r\nData was examined using Tableau’s data viewer after arranging data in proposed visualisation. This step is repeated twice for both files. Data view was exported to .csv format\r\n\r\nAfter checking both files, a new excel (final.xlsx) was created containing the pivoted versions of the Labour force and LFPR\r\n\r\nVisualisation Process in Tableau is listed in the following section\r\nSteps\r\nVisualisation\r\nData was imported into Tableau\r\n\r\nFor the first bar chart titled ‘Labour Force by Years 2009,2019’, Category,Age Band and Year were placed into Columns while sum(value) was placed into rows. Bars were coloured according to Year Values\r\n\r\nNext, to hide unnecessary data, Category and Age Band was placed into filters to display current visualisation\r\n\r\nTo highlight the change between the 2 years, a line chart was added, this was done by placing sum(value) in Row and creating a dual axis.Plot was coloured by sum(value), to make the line bar a single color, I edit the color values to be a 2-step color band and changed the minimum value to allow the bar to be colored dark red\r\n\r\nFinally, annotations were added to highlight derived insights.\r\n\r\nFor the dumb bell chart, similar steps was used as above, the main difference is that the circle option was selected for the visualisation.\r\n\r\n4. Observations from Makeover Visualisation \r\nFIGURE 3:FINAL DASHBOARD VISUALISATIONFrom the makeover done on the visualisation. We can derive clearer insights compared to the original graph.\r\nFrom the Labour Force by Years 2009,2019 Plot, there is a >22% increase in labour force aged 45&above when comparing 2019 with 2009. At a glance, this conveys to the audience the presence of an aging population in singapore.\r\nFrom the Labour Force Participation Rate by Years 2009,2019 Plot, the largest % increase in labour force participation rate was seen in the residents aged 65&over. This could be attirbuted to the increase of the retirement age from 65 to 67 in 2017 (Source: https://www.mom.gov.sg/employment-practices/retirement)\r\nBoth the 15-24 and 25-54 age bands show lesser increases in LFPR, this signifies that there could be low birth rates within the span of 10 years as the changes are not as significant compared to older workers\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-01-28T18:27:34+08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Re-presenting Data",
    "description": "The world forgetting by the world forgot. Eternal Sunshine of the Spotless Mind!",
    "author": [
      {
        "name": "Atticus Foo",
        "url": "atticusfoo.2020@mitb.smu.edu.sg"
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-01T20:12:29+08:00",
    "input_file": {}
  }
]
